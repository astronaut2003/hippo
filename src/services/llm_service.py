"""
LLM æœåŠ¡å°è£…
å¤„ç†ä¸ OpenAI API çš„äº¤äº’
"""
from openai import AsyncOpenAI
from typing import List, Dict, AsyncGenerator
import logging

logger = logging.getLogger(__name__)


class LLMService:
    """LLM è°ƒç”¨æœåŠ¡"""
    
    def __init__(self, api_key: str, model: str = "Qwen/Qwen2.5-72B-Instruct", base_url: str = None):
        """
        åˆå§‹åŒ– LLM æœåŠ¡ï¼Œæ”¯æŒç¡…åŸºæµåŠ¨å¹³å°
        Args:
            api_key: API Key
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            base_url: API Base URL (ç¡…åŸºæµåŠ¨å¹³å°)
        """
        if base_url:
            self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)
            logger.info(f"âœ… LLM æœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {model}, base_url: {base_url}")
        else:
            self.client = AsyncOpenAI(api_key=api_key)
            logger.info(f"âœ… LLM æœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {model}")
        self.model = model
    
    async def chat_stream(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆå¯¹è¯ï¼Œå…¼å®¹ reasoning_content å­—æ®µ
        """
        try:
            logger.info(f"ğŸ¤– å¼€å§‹æµå¼ç”Ÿæˆï¼Œæ¶ˆæ¯æ•°: {len(messages)}")
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )
            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, "content") and delta.content:
                        yield delta.content
                    if hasattr(delta, "reasoning_content") and delta.reasoning_content:
                        yield delta.reasoning_content
            logger.info("âœ… æµå¼ç”Ÿæˆå®Œæˆ")
                    
        except Exception as e:
            logger.error(f"âŒ LLM ç”Ÿæˆå¤±è´¥: {e}")
            yield f"\n\n[é”™è¯¯: {str(e)}]"
    
    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> str:
        """
        éæµå¼ç”Ÿæˆå¯¹è¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
        
        Returns:
            ç”Ÿæˆçš„å›å¤æ–‡æœ¬
        """
        try:
            logger.info(f"ğŸ¤– å¼€å§‹ç”Ÿæˆå›å¤ï¼Œæ¶ˆæ¯æ•°: {len(messages)}")
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            content = response.choices[0].message.content
            logger.info(f"âœ… ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(content)}")
            return content
            
        except Exception as e:
            logger.error(f"âŒ LLM ç”Ÿæˆå¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
