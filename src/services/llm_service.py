"""
LLM æœåŠ¡å°è£…
å¤„ç†ä¸ OpenAI API çš„äº¤äº’
"""
from openai import AsyncOpenAI
from typing import List, Dict, AsyncGenerator
import logging

logger = logging.getLogger(__name__)


class LLMService:
    """LLM è°ƒç”¨æœåŠ¡"""
    
    def __init__(self, api_key: str, model: str = "gpt-4"):
        """
        åˆå§‹åŒ– LLM æœåŠ¡
        
        Args:
            api_key: OpenAI API Key
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
        logger.info(f"âœ… LLM æœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {model}")
    
    async def chat_stream(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆå¯¹è¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•° (0-2)
            max_tokens: æœ€å¤§ token æ•°
        
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            logger.info(f"ğŸ¤– å¼€å§‹æµå¼ç”Ÿæˆï¼Œæ¶ˆæ¯æ•°: {len(messages)}")
            
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
            
            logger.info("âœ… æµå¼ç”Ÿæˆå®Œæˆ")
                    
        except Exception as e:
            logger.error(f"âŒ LLM ç”Ÿæˆå¤±è´¥: {e}")
            yield f"\n\n[é”™è¯¯: {str(e)}]"
    
    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> str:
        """
        éæµå¼ç”Ÿæˆå¯¹è¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
        
        Returns:
            ç”Ÿæˆçš„å›å¤æ–‡æœ¬
        """
        try:
            logger.info(f"ğŸ¤– å¼€å§‹ç”Ÿæˆå›å¤ï¼Œæ¶ˆæ¯æ•°: {len(messages)}")
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            content = response.choices[0].message.content
            logger.info(f"âœ… ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(content)}")
            return content
            
        except Exception as e:
            logger.error(f"âŒ LLM ç”Ÿæˆå¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"
