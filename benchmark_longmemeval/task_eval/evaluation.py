"""
LongMemEval è¯„ä¼°æ¨¡å—ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

å‚è€ƒ zep-longmem-eval çš„è¯„ä¼°è®¾è®¡ï¼Œç®€åŒ–æç¤ºè¯ï¼Œç§»é™¤å¯¹æŠ—æ€§é—®é¢˜æ”¯æŒã€‚
é€‚é…æ–°çš„ LongMemEval æ•°æ®é›†ç»“æ„ã€‚

ä¸»è¦æ”¹è¿›ï¼š
1. ç®€åŒ– LLM grader æç¤ºè¯ï¼Œæ›´å®½æ¾çš„è¯„åˆ†æ ‡å‡†
2. ç§»é™¤å¯¹æŠ—æ€§é—®é¢˜çš„è¯„ä¼°é€»è¾‘
3. ç»Ÿä¸€è¯„åˆ†æ¥å£ï¼Œç®€åŒ– API
4. ä¿æŒåŸæœ‰çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆEM, F1, ROUGE, BLEU, METEOR, Semantic Similarity, BERT F1ï¼‰
"""

from datetime import datetime
import regex
import json
import string
import unicodedata
from typing import List, Dict, Any, Optional
import numpy as np
from collections import Counter
import logging

# è¯„ä¼°ç›¸å…³åº“
from bert_score import score
from nltk.stem import PorterStemmer
from rouge import Rouge
import nltk
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
from scipy.spatial.distance import cosine
from sentence_transformers import SentenceTransformer

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))
from llm_client import LLMClient

# å°è¯•ä¸‹è½½NLTKèµ„æº
# try:
#     nltk.download("wordnet", quiet=True)
#     nltk.download("punkt", quiet=True)
# except Exception as e:
#     logging.warning(f"Failed to download NLTK resources: {e}")

# åˆå§‹åŒ–è¯å¹²æå–å™¨
ps = PorterStemmer()

# ================================
# ZEP-style Grading Promptsï¼ˆä¸åŒé—®é¢˜ç±»å‹ä½¿ç”¨ä¸åŒæç¤ºè¯ï¼‰
# ================================

GRADING_PROMPTS = {
    "temporal-reasoning": """
    I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains a subset of the information required by the answer, answer no. In addition, do not penalize off-by-one errors for the number of days. If the question asks for the number of days/weeks/months, etc., and the model makes off-by-one errors (e.g., predicting 19 days when the answer is 18), the model's response is still correct.

    <QUESTION>
    B: {question}
    </QUESTION>
    <CORRECT ANSWER>
    {gold_answer}
    </CORRECT ANSWER>
    <RESPONSE>
    A: {response}
    </RESPONSE>
    """,
    "knowledge-update": """
    I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response contains some previous information along with an updated answer, the response should be considered as correct as long as the updated answer is the required answer.

    <QUESTION>
    B: {question}
    </QUESTION>
    <CORRECT ANSWER>
    {gold_answer}
    </CORRECT ANSWER>
    <RESPONSE>
    A: {response}
    </RESPONSE>
    """,
    "single-session-preference": """
    I will give you a question, a rubric for desired personalized response, and a response from a model. Please answer yes if the response satisfies the desired response. Otherwise, answer no. The model does not need to reflect all the points in the rubric. The response is correct as long as it recalls and utilizes the user's personal information correctly.

    <QUESTION>
    B: {question}
    </QUESTION>
    <RUBRIC>
    {gold_answer}
    </RUBRIC>
    <RESPONSE>
    A: {response}
    </RESPONSE>
    """,
    "default": """
    I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains a subset of the information required by the answer, answer no.

    <QUESTION>
    B: {question}
    </QUESTION>
    <CORRECT ANSWER>
    {gold_answer}
    </CORRECT ANSWER>
    <RESPONSE>
    A: {response}
    </RESPONSE>
    """,
}

# ================================
# æ¨¡å‹ç®¡ç†å™¨ - é¿å…é‡å¤åˆå§‹åŒ–
# ================================

class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†å„ç§è¯„ä¼°æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½"""
    
    def __init__(self):
        self._models: Dict[str, Any] = {}
        self.logger = logging.getLogger(f"{__name__}.ModelManager")
    
    def get_sentence_model(self, model_name: str = "all-MiniLM-L6-v2") -> Optional[SentenceTransformer]:
        """è·å–å¥å­åµŒå…¥æ¨¡å‹"""
        cache_key = f"sentence_transformer:{model_name}"
        
        if cache_key in self._models:
            return self._models[cache_key]
        
        try:
            self.logger.info(f"æ­£åœ¨åŠ è½½å¥å­åµŒå…¥æ¨¡å‹: {model_name}")
            model = SentenceTransformer(model_name)
            self._models[cache_key] = model
            self.logger.info(f"âœ… å¥å­åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            return model
        except Exception as e:
            self.logger.error(f"åŠ è½½å¥å­åµŒå…¥æ¨¡å‹å¤±è´¥ {model_name}: {e}")
            self._models[cache_key] = None
            return None
    
    def get_bert_score_model(self) -> bool:
        """æ£€æŸ¥BERTScoreæ¨¡å‹æ˜¯å¦å¯ç”¨"""
        cache_key = "bert_score_available"
        
        if cache_key in self._models:
            return self._models[cache_key]
        
        try:
            from bert_score import score as bert_score
            _, _, f1 = bert_score(["test"], ["test"], lang="en", verbose=False)
            self._models[cache_key] = True
            self.logger.info("âœ… BERTScoreæ¨¡å‹å¯ç”¨")
            return True
        except Exception as e:
            self.logger.warning(f"BERTScoreæ¨¡å‹ä¸å¯ç”¨: {e}")
            self._models[cache_key] = False
            return False
    
    def clear_cache(self):
        """æ¸…ç©ºæ¨¡å‹ç¼“å­˜"""
        for cache_key, model in self._models.items():
            try:
                if hasattr(model, 'cpu'):
                    model.cpu()
                if hasattr(model, 'cleanup'):
                    model.cleanup()
            except Exception as e:
                self.logger.warning(f"æ¸…ç†æ¨¡å‹ {cache_key} å¤±è´¥: {e}")
        
        self._models.clear()
        
        # æ¸…ç†GPUç¼“å­˜
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except ImportError:
            pass
        
        self.logger.info("è¯„ä¼°æ¨¡å‹ç¼“å­˜å·²æ¸…ç©º")

# å…¨å±€æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
_model_manager: Optional[ModelManager] = None

def get_model_manager() -> ModelManager:
    """è·å–å…¨å±€æ¨¡å‹ç®¡ç†å™¨ï¼ˆå•ä¾‹ï¼‰"""
    global _model_manager
    if _model_manager is None:
        _model_manager = ModelManager()
    return _model_manager

def cleanup_evaluation_models():
    """æ¸…ç†è¯„ä¼°æ¨¡å‹ç¼“å­˜"""
    global _model_manager
    if _model_manager:
        _model_manager.clear_cache()
        _model_manager = None

# ================================
# LLM è¯„ä¼°å™¨ï¼ˆç®€åŒ–ç‰ˆï¼Œå‚è€ƒ zep-longmem-evalï¼‰
# ================================

def llm_grader(llm_client: LLMClient, 
               question: str, 
               gold_answer: str, 
               response: str,
               question_type: str = "default",
               context: str = "") -> bool:
    """
    ä½¿ç”¨ LLM åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼ˆZEP-styleï¼‰
    
    å®Œå…¨å¯¹é½ ZEP çš„è¯„ä¼°é€»è¾‘ï¼š
    - æ ¹æ®é—®é¢˜ç±»å‹ä½¿ç”¨ä¸åŒçš„ grading prompt
    - ä¸¥æ ¼çš„ yes/no åˆ¤æ–­æ ‡å‡†
    - æ”¯æŒ temporal-reasoning, knowledge-update, single-session-preference ç­‰ç±»å‹
    
    Args:
        llm_client: LLMå®¢æˆ·ç«¯
        question: é—®é¢˜æ–‡æœ¬
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        question_type: é—®é¢˜ç±»å‹ï¼ˆtemporal-reasoning, knowledge-update, single-session-preference, defaultï¼‰
        context: ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™å‚æ•°ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼Œä½†ZEPä¸ä½¿ç”¨ï¼‰
        
    Returns:
        æ˜¯å¦æ­£ç¡®ï¼ˆTrue/Falseï¼‰
    """
    
    # ğŸ”¥ ZEP-style: æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹© prompt
    prompt_template = GRADING_PROMPTS.get(question_type, GRADING_PROMPTS["default"])
    prompt = prompt_template.format(
        question=question,
        gold_answer=gold_answer,
        response=response
    )
    
    # System promptï¼ˆä¸ZEPä¿æŒä¸€è‡´ï¼‰
    system_prompt = "You are an expert grader that determines if answers to questions match a gold standard answer"

    try:
        # ğŸ”¥ å°è¯•ä½¿ç”¨ JSON æ ¼å¼ï¼ˆæ¨¡æ‹Ÿ ZEP çš„ç»“æ„åŒ–è¾“å‡ºï¼‰
        full_prompt = f"""{system_prompt}

        {prompt}

        Return ONLY a JSON object with "is_correct" key containing "yes" or "no".
        Example: {{"is_correct": "yes"}} or {{"is_correct": "no"}}
        """
        
        llm_response = llm_client.generate_answer(
            prompt=full_prompt,
            temperature=0.0,
            max_tokens=50,
            json_format=True
        )
        
        # è§£æ JSON å“åº”
        try:
            if '{' in llm_response and '}' in llm_response:
                start = llm_response.find('{')
                end = llm_response.rfind('}') + 1
                json_str = llm_response[start:end]
                result = json.loads(json_str)
                
                # ZEP ä½¿ç”¨ "is_correct" å­—æ®µ
                is_correct = result.get("is_correct", "").strip().lower()
                return is_correct == "yes"
            else:
                # å›é€€åˆ°æ–‡æœ¬è§£æ
                llm_response_lower = llm_response.lower().strip()
                
                # ZEP-style: ä¸¥æ ¼åŒ¹é… "yes" æˆ– "no"
                if llm_response_lower == "yes":
                    return True
                elif llm_response_lower == "no":
                    return False
                
                # å¦‚æœå“åº”ä¸­åŒ…å« yes/noï¼Œæå–ç¬¬ä¸€ä¸ªå‡ºç°çš„
                if "yes" in llm_response_lower and "no" not in llm_response_lower:
                    return True
                elif "no" in llm_response_lower and "yes" not in llm_response_lower:
                    return False
                else:
                    # é»˜è®¤è¿”å› Falseï¼ˆä¸¥æ ¼è¯„åˆ†ï¼‰
                    logging.warning(f"Ambiguous LLM response: {llm_response}")
                    return False
                    
        except json.JSONDecodeError as e:
            logging.warning(f"JSONè§£æå¤±è´¥: {llm_response}, é”™è¯¯: {e}")
            
            # å›é€€ï¼šä¸¥æ ¼çš„ yes/no åŒ¹é…
            llm_response_lower = llm_response.lower().strip()
            
            if llm_response_lower == "yes":
                return True
            elif llm_response_lower == "no":
                return False
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å« yes æˆ– no
            contains_yes = "yes" in llm_response_lower
            contains_no = "no" in llm_response_lower
            
            if contains_yes and not contains_no:
                return True
            elif contains_no and not contains_yes:
                return False
            else:
                # ZEP-style: é»˜è®¤ä¸¥æ ¼è¯„åˆ†ï¼Œè¿”å› False
                logging.warning(f"Cannot parse LLM response: {llm_response}")
                return False
                
    except Exception as e:
        logging.error(f"LLM grader å¤±è´¥: {e}")
        return False

def calculate_llm_judgment(llm_client: LLMClient, 
                        question: str, 
                        gold_answer: str, 
                        response: str,
                        question_type: str = "default",
                        num_runs: int = 1,
                        context: str = "") -> Dict[str, Any]:
    """
    è®¡ç®—LLMåˆ¤æ–­åˆ†æ•°ï¼ˆæ”¯æŒå¤šæ¬¡è¿è¡Œï¼ŒZEP-styleï¼‰
    
    Args:
        llm_client: LLMå®¢æˆ·ç«¯
        question: é—®é¢˜
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆç­”æ¡ˆ
        question_type: é—®é¢˜ç±»å‹ï¼ˆZEP-styleï¼‰
        num_runs: è¿è¡Œæ¬¡æ•°ï¼ˆç”¨äºä¸€è‡´æ€§æ£€æŸ¥ï¼‰
        context: ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™ä»¥å…¼å®¹ï¼Œä½†ZEPä¸ä½¿ç”¨ï¼‰
        
    Returns:
        LLMåˆ¤æ–­ç»“æœå­—å…¸
    """
    judgments = []
    
    for i in range(num_runs):
        try:
            result = llm_grader(llm_client, question, gold_answer, response, question_type, context)
            judgments.append(result)
        except Exception as e:
            logging.warning(f"LLMåˆ¤æ–­ç¬¬ {i+1} æ¬¡å¤±è´¥: {e}")
            continue
    
    if not judgments:
        return {
            "judgments": [],
            "accuracy": 0.0,
            "num_runs": num_runs,
            "consistency": False,
            "question_type": question_type,
            "error": "æ‰€æœ‰åˆ¤æ–­éƒ½å¤±è´¥äº†"
        }
    
    accuracy = sum(judgments) / len(judgments)
    consistency = len(set(judgments)) == 1
    
    return {
        "judgments": judgments,
        "accuracy": accuracy,
        "num_runs": num_runs,
        "consistency": consistency,
        "confidence": "high" if consistency else "low",
        "question_type": question_type,
        "context_provided": bool(context and context.strip())
    }

# ================================
# ç»¼åˆè¯„ä¼°å‡½æ•°
# ================================

def calculate_comprehensive_scores(gold_answer: str, 
                                 response: str, 
                                 question: str = "", 
                                 context: str = "",
                                 question_type: str = "default",  # ğŸ”¥ æ–°å¢å‚æ•°
                                 llm_client: Optional[LLMClient] = None,
                                 metrics: Optional[List[str]] = None,
                                 sentence_model_name: str = "all-MiniLM-L6-v2") -> Dict[str, Any]:
    """
    è®¡ç®—å…¨é¢çš„è¯„ä¼°åˆ†æ•°ï¼ˆZEP-compatibleï¼‰
    
    Args:
        gold_answer: æ ‡å‡†ç­”æ¡ˆ
        response: ç”Ÿæˆçš„ç­”æ¡ˆ
        question: é—®é¢˜æ–‡æœ¬
        context: ä¸Šä¸‹æ–‡
        question_type: é—®é¢˜ç±»å‹ï¼ˆZEP-styleï¼‰
        llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        metrics: è¦è®¡ç®—çš„æŒ‡æ ‡åˆ—è¡¨
        sentence_model_name: å¥å­åµŒå…¥æ¨¡å‹åç§°
        
    Returns:
        åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    
    # é»˜è®¤æŒ‡æ ‡
    if llm_client is not None and metrics is None:
        metrics = ["exact_match", "f1", "rouge", "bleu", "meteor", "semantic_similarity", "bert_f1", "llm_judge"]
    if metrics is None:
        metrics = ["exact_match", "f1", "rouge", "bleu", "meteor", "semantic_similarity", "bert_f1"]
    
    # æ•°æ®é¢„å¤„ç†
    gold_answer = str(gold_answer).strip() if gold_answer else ""
    response = str(response).strip() if response else ""
    
    results = {
        "input_info": {
            "gold_length": len(gold_answer.split()),
            "response_length": len(response.split()),
            "context_length": len(context.split()) if context else 0,
            "question_type": question_type  # ğŸ”¥ æ·»åŠ é—®é¢˜ç±»å‹ä¿¡æ¯
        },
        "scores": {}
    }
    
    # åŸºç¡€æŒ‡æ ‡ï¼ˆä¿æŒä¸å˜ï¼‰
    if "exact_match" in metrics:
        try:
            results["scores"]["exact_match"] = float(exact_match_score(gold_answer, response))
        except Exception as e:
            logging.warning(f"ç²¾ç¡®åŒ¹é…è®¡ç®—å¤±è´¥: {e}")
            results["scores"]["exact_match"] = 0.0
    
    if "f1" in metrics:
        try:
            results["scores"]["token_f1"] = calculate_f1_score(gold_answer, response)
        except Exception as e:
            logging.warning(f"F1è®¡ç®—å¤±è´¥: {e}")
            results["scores"]["token_f1"] = 0.0
    
    if "rouge" in metrics:
        try:
            rouge_scores = calculate_rouge_score(gold_answer, response)
            results["scores"].update(rouge_scores)
        except Exception as e:
            logging.warning(f"ROUGEè®¡ç®—å¤±è´¥: {e}")
            results["scores"].update({"rouge1_f": 0.0, "rouge2_f": 0.0, "rougeL_f": 0.0})
    
    if "bleu" in metrics:
        try:
            bleu_scores = calculate_bleu_score(gold_answer, response)
            results["scores"].update(bleu_scores)
        except Exception as e:
            logging.warning(f"BLEUè®¡ç®—å¤±è´¥: {e}")
            results["scores"].update({"bleu1": 0.0, "bleu2": 0.0, "bleu3": 0.0, "bleu4": 0.0})
    
    if "meteor" in metrics:
        try:
            results["scores"]["meteor"] = calculate_meteor_score(gold_answer, response)
        except Exception as e:
            logging.warning(f"METEORè®¡ç®—å¤±è´¥: {e}")
            results["scores"]["meteor"] = 0.0
    
    if "semantic_similarity" in metrics:
        try:
            results["scores"]["semantic_similarity"] = calculate_semantic_similarity(
                gold_answer, response, sentence_model_name
            )
        except Exception as e:
            logging.warning(f"è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            results["scores"]["semantic_similarity"] = 0.0
    
    if "bert_f1" in metrics:
        try:
            results["scores"]["bert_f1"] = calculate_bert_f1_score(gold_answer, response)
        except Exception as e:
            logging.warning(f"BERT F1è®¡ç®—å¤±è´¥: {e}")
            results["scores"]["bert_f1"] = 0.0
    
    # ğŸ”¥ LLMè¯„ä¼°ï¼ˆZEP-styleï¼Œä½¿ç”¨ question_typeï¼‰
    if llm_client and question and "llm_judge" in metrics:
        try:
            llm_result = calculate_llm_judgment(
                llm_client, question, gold_answer, response, 
                question_type=question_type,  # ğŸ”¥ ä¼ é€’é—®é¢˜ç±»å‹
                num_runs=1, 
                context=context
            )
            results["scores"]["llm_accuracy"] = llm_result["accuracy"]
            results["llm_details"] = llm_result
        except Exception as e:
            logging.warning(f"LLMè¯„ä¼°å¤±è´¥: {e}")
            results["scores"]["llm_accuracy"] = 0.0
            results["llm_details"] = {"error": str(e)}
    
    # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
    try:
        lexical_scores = []
        semantic_scores = []
        
        for key in ["exact_match", "token_f1", "rouge1_f", "rougeL_f", "bleu4", "meteor"]:
            if key in results["scores"]:
                lexical_scores.append(results["scores"][key])
        
        for key in ["semantic_similarity", "bert_f1"]:
            if key in results["scores"]:
                semantic_scores.append(results["scores"][key])
        
        if lexical_scores:
            results["scores"]["avg_lexical"] = sum(lexical_scores) / len(lexical_scores)
        if semantic_scores:
            results["scores"]["avg_semantic"] = sum(semantic_scores) / len(semantic_scores)
        
        all_scores = lexical_scores + semantic_scores
        if all_scores:
            results["scores"]["overall_average"] = sum(all_scores) / len(all_scores)
            
    except Exception as e:
        logging.warning(f"ç»¼åˆåˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
    
    results = convert_numpy_types(results)
    results["evaluation_success"] = True
    
    return results

# ================================
# æ‰¹é‡è¯„ä¼°
# ================================

def batch_evaluate(questions: List[str],
                  gold_answers: List[str], 
                  predicted_answers: List[str],
                  contexts: Optional[List[str]] = None,
                  llm_client: Optional[LLMClient] = None,
                  metrics: Optional[List[str]] = None,
                  include_individual: bool = False,
                  sentence_model_name: str = "all-MiniLM-L6-v2") -> Dict[str, Any]:
    """
    æ‰¹é‡è¯„ä¼°å¤šä¸ªé—®ç­”å¯¹
    
    Args:
        questions: é—®é¢˜åˆ—è¡¨
        gold_answers: æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨
        predicted_answers: é¢„æµ‹ç­”æ¡ˆåˆ—è¡¨
        contexts: ä¸Šä¸‹æ–‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        metrics: è¦è®¡ç®—çš„æŒ‡æ ‡åˆ—è¡¨
        include_individual: æ˜¯å¦åŒ…å«å•ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœ
        sentence_model_name: å¥å­åµŒå…¥æ¨¡å‹åç§°
        
    Returns:
        åŒ…å«æ‰¹é‡è¯„ä¼°ç»“æœçš„å­—å…¸
    """
    if not (len(questions) == len(gold_answers) == len(predicted_answers)):
        raise ValueError("è¾“å…¥åˆ—è¡¨é•¿åº¦ä¸ä¸€è‡´")
    
    if contexts is None:
        contexts = [""] * len(questions)
    elif len(contexts) != len(questions):
        raise ValueError("ä¸Šä¸‹æ–‡åˆ—è¡¨é•¿åº¦ä¸é—®é¢˜åˆ—è¡¨ä¸ä¸€è‡´")
    
    results = {
        "summary": {
            "total_samples": len(questions),
            "evaluation_metrics": metrics or ["exact_match", "f1", "rouge", "bleu", "meteor", "semantic_similarity", "bert_f1"],
            "timestamp": datetime.now().isoformat(),
            "sentence_model": sentence_model_name
        },
        "aggregate_scores": {},
        "individual_results": [] if include_individual else None
    }
    
    # é¢„åŠ è½½æ¨¡å‹
    manager = get_model_manager()
    if "semantic_similarity" in (metrics or []):
        manager.get_sentence_model(sentence_model_name)
    if "bert_f1" in (metrics or []):
        manager.get_bert_score_model()
    
    # æ”¶é›†æ‰€æœ‰è¯„ä¼°ç»“æœ
    all_scores = []
    failed_count = 0
    
    for i, (question, gold_answer, predicted_answer, context) in enumerate(
        zip(questions, gold_answers, predicted_answers, contexts)
    ):
        try:
            eval_result = calculate_comprehensive_scores(
                gold_answer=gold_answer,
                response=predicted_answer,
                question=question,
                context=context,
                llm_client=llm_client,
                metrics=metrics,
                sentence_model_name=sentence_model_name
            )
            
            all_scores.append(eval_result["scores"])
            
            if include_individual:
                results["individual_results"].append({
                    "index": i,
                    "question": question,
                    "gold_answer": gold_answer,
                    "predicted_answer": predicted_answer,
                    "evaluation": eval_result
                })
                
        except Exception as e:
            logging.error(f"è¯„ä¼°ç¬¬{i+1}ä¸ªæ ·æœ¬å¤±è´¥: {e}")
            failed_count += 1
            
            if include_individual:
                results["individual_results"].append({
                    "index": i,
                    "question": question,
                    "gold_answer": gold_answer,
                    "predicted_answer": predicted_answer,
                    "evaluation": {"error": str(e)}
                })
        
        # è¿›åº¦è¾“å‡º
        if (i + 1) % 100 == 0:
            logging.info(f"æ‰¹é‡è¯„ä¼°è¿›åº¦: {i + 1}/{len(questions)} ({(i + 1)/len(questions)*100:.1f}%)")
    
    # è®¡ç®—èšåˆç»Ÿè®¡
    if all_scores:
        metric_values = {}
        for score_dict in all_scores:
            for metric_name, value in score_dict.items():
                if isinstance(value, (int, float)):
                    if metric_name not in metric_values:
                        metric_values[metric_name] = []
                    metric_values[metric_name].append(value)
        
        for metric_name, values in metric_values.items():
            if values:
                results["aggregate_scores"][metric_name] = {
                    "mean": sum(values) / len(values),
                    "std": np.std(values).item() if len(values) > 1 else 0.0,
                    "min": min(values),
                    "max": max(values),
                    "median": np.median(values).item(),
                    "count": len(values)
                }
    
    results["summary"]["failed_evaluations"] = failed_count
    results["summary"]["success_rate"] = (len(questions) - failed_count) / len(questions) if questions else 0.0
    
    return results

# ================================
# åŸºç¡€è¯„ä¼°å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
# ================================

def calculate_semantic_similarity(gold_answer: str, 
                                response: str, 
                                model_name: str = "all-MiniLM-L6-v2") -> float:
    """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    response = str(response) if response is not None else ""
    
    if not gold_answer.strip() or not response.strip():
        return 0.0
    
    try:
        sentence_model = get_model_manager().get_sentence_model(model_name)
        if sentence_model is None:
            return 0.0
            
        gold_embedding = sentence_model.encode([gold_answer], show_progress_bar=False)[0]
        response_embedding = sentence_model.encode([response], show_progress_bar=False)[0]
        similarity = 1 - cosine(gold_embedding, response_embedding)
        
        return max(0.0, min(1.0, similarity))
        
    except Exception as e:
        logging.error(f"Failed to calculate semantic similarity: {e}")
        return 0.0

def calculate_bert_f1_score(gold_answer: str, response: str) -> float:
    """è®¡ç®—BERT F1åˆ†æ•°"""
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    response = str(response) if response is not None else ""
    
    if not gold_answer.strip() or not response.strip():
        return 0.0
    
    try:
        manager = get_model_manager()
        if not manager.get_bert_score_model():
            return 0.0
        
        _, _, f1 = score([response], [gold_answer], lang="en", rescale_with_baseline=True, verbose=False)
        return f1.item() if f1 is not None else 0.0
    except Exception as e:
        logging.error(f"Failed to calculate BERT F1 score: {e}")
        return 0.0

class SimpleTokenizer(object):
    """ç®€å•çš„åˆ†è¯å™¨ç±»"""
    ALPHA_NUM = r'[\p{L}\p{N}\p{M}]+'
    NON_WS = r'[^\p{Z}\p{C}]'

    def __init__(self):
        self._regexp = regex.compile(
            '(%s)|(%s)' % (self.ALPHA_NUM, self.NON_WS),
            flags=regex.IGNORECASE + regex.UNICODE + regex.MULTILINE
        )

    def tokenize(self, text, uncased=False):
        matches = [m for m in self._regexp.finditer(text)]
        if uncased:
            tokens = [m.group().lower() for m in matches]
        else:
            tokens = [m.group() for m in matches]
        return tokens

def normalize_answer(s):
    """ç­”æ¡ˆæ ‡å‡†åŒ–"""
    if s is None:
        s = ""
    elif not isinstance(s, str):
        s = str(s)
    
    s = s.replace(',', "")
    
    def remove_articles(text):
        return regex.sub(r'\b(a|an|the|and)\b', ' ', text)

    def white_space_fix(text):
        return ' '.join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))

def exact_match_score(gold_answer: str, response: str) -> bool:
    """ç²¾ç¡®åŒ¹é…å¾—åˆ†"""
    response = str(response) if response is not None else ""
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    
    response = normalize_answer(response)
    gold_answer = normalize_answer(gold_answer)
    return set(response.split()) == set(gold_answer.split())

def calculate_f1_score(gold_answer: str, response: str) -> float:
    """F1å¾—åˆ†"""
    response = str(response) if response is not None else ""
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    
    response_tokens = [ps.stem(w) for w in normalize_answer(response).split()]
    gold_answer_tokens = [ps.stem(w) for w in normalize_answer(gold_answer).split()]
    
    common = Counter(response_tokens) & Counter(gold_answer_tokens)
    num_same = sum(common.values())
    
    if num_same == 0:
        return 0
    
    precision = 1.0 * num_same / len(response_tokens)
    recall = 1.0 * num_same / len(gold_answer_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    
    return f1

def calculate_rouge_score(gold_answer: str, response: str) -> Dict[str, float]:
    """è®¡ç®—ROUGEåˆ†æ•°"""
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    response = str(response) if response is not None else ""
    
    metrics = {"rouge1_f": 0.0, "rouge2_f": 0.0, "rougeL_f": 0.0}
    
    try:
        scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
        rouge_scores = scorer.score(gold_answer, response)
        metrics["rouge1_f"] = rouge_scores["rouge1"].fmeasure
        metrics["rouge2_f"] = rouge_scores["rouge2"].fmeasure
        metrics["rougeL_f"] = rouge_scores["rougeL"].fmeasure
    except Exception as e:
        logging.error(f"Failed to calculate ROUGE scores: {e}")
    
    return metrics

def calculate_bleu_score(gold_answer: str, response: str) -> Dict[str, float]:
    """è®¡ç®—BLEUåˆ†æ•°"""
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    response = str(response) if response is not None else ""
    
    metrics = {"bleu1": 0.0, "bleu2": 0.0, "bleu3": 0.0, "bleu4": 0.0}

    try:
        gold_tokens = nltk.word_tokenize(gold_answer.lower())
        response_tokens = nltk.word_tokenize(response.lower())
        
        smoothing = SmoothingFunction().method1
        weights = [(1, 0, 0, 0), (0.5, 0.5, 0, 0), (0.33, 0.33, 0.33, 0), (0.25, 0.25, 0.25, 0.25)]

        for i, weight in enumerate(weights, 1):
            metrics[f"bleu{i}"] = sentence_bleu(
                [gold_tokens], response_tokens, weights=weight, smoothing_function=smoothing
            )
    except Exception as e:
        logging.error(f"Failed to calculate BLEU scores: {e}")

    return metrics

def calculate_meteor_score(gold_answer: str, response: str) -> float:
    """è®¡ç®—METEORåˆ†æ•°"""
    gold_answer = str(gold_answer) if gold_answer is not None else ""
    response = str(response) if response is not None else ""
    
    try:
        gold_tokens = nltk.word_tokenize(gold_answer.lower())
        response_tokens = nltk.word_tokenize(response.lower())
        return meteor_score([gold_tokens], response_tokens)
    except Exception as e:
        logging.error(f"Failed to calculate METEOR score: {e}")
        return 0.0

def convert_numpy_types(obj):
    """è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
    if isinstance(obj, np.number):
        return float(obj)
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(i) for i in obj]
    else:
        return obj

# ================================
# æŠ¥å‘Šç”Ÿæˆ
# ================================

def generate_evaluation_report(eval_results: Dict[str, Any], 
                             output_format: str = "text",
                             save_path: Optional[str] = None) -> str:
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    if output_format == "json":
        report = json.dumps(eval_results, indent=2, ensure_ascii=False)
    elif output_format == "markdown":
        report = _generate_markdown_report(eval_results)
    else:
        report = _generate_text_report(eval_results)
    
    if save_path:
        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report)
            logging.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
        except Exception as e:
            logging.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    return report

def _generate_text_report(eval_results: Dict[str, Any]) -> str:
    """ç”Ÿæˆæ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
    lines = []
    lines.append("="*60)
    lines.append("LongMemEval è¯„ä¼°æŠ¥å‘Š")
    lines.append("="*60)
    
    if "summary" in eval_results:
        summary = eval_results["summary"]
        lines.append(f"æ€»æ ·æœ¬æ•°: {summary.get('total_samples', 'unknown')}")
        lines.append(f"æˆåŠŸç‡: {summary.get('success_rate', 0):.2%}")
        lines.append(f"å¤±è´¥æ•°: {summary.get('failed_evaluations', 0)}")
        lines.append("")
    
    if "aggregate_scores" in eval_results:
        lines.append("èšåˆè¯„ä¼°ç»“æœ:")
        lines.append("-" * 40)
        
        for metric_name, stats in eval_results["aggregate_scores"].items():
            lines.append(f"{metric_name:20} | å‡å€¼: {stats['mean']:.4f} | æ ‡å‡†å·®: {stats['std']:.4f}")
        lines.append("")
    
    return "\n".join(lines)

def _generate_markdown_report(eval_results: Dict[str, Any]) -> str:
    """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
    lines = []
    lines.append("# LongMemEval è¯„ä¼°æŠ¥å‘Š")
    lines.append("")
    
    if "summary" in eval_results:
        summary = eval_results["summary"]
        lines.append("## åŸºæœ¬ä¿¡æ¯")
        lines.append(f"- **æ€»æ ·æœ¬æ•°**: {summary.get('total_samples', 'unknown')}")
        lines.append(f"- **æˆåŠŸç‡**: {summary.get('success_rate', 0):.2%}")
        lines.append("")
    
    if "aggregate_scores" in eval_results:
        lines.append("## èšåˆè¯„ä¼°ç»“æœ")
        lines.append("")
        lines.append("| æŒ‡æ ‡ | å‡å€¼ | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ | ä¸­ä½æ•° |")
        lines.append("|------|------|--------|--------|--------|--------|")
        
        for metric_name, stats in eval_results["aggregate_scores"].items():
            lines.append(f"| {metric_name} | {stats['mean']:.4f} | {stats['std']:.4f} | {stats['min']:.4f} | {stats['max']:.4f} | {stats['median']:.4f} |")
        lines.append("")
    
    return "\n".join(lines)

# ================================
# æµ‹è¯•ä»£ç 
# ================================

if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    print("ğŸ§ª æµ‹è¯•è¯„ä¼°æ¨¡å—...")
    
    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    llm_client = LLMClient("deepseek-chat")
    
    # æµ‹è¯•ç”¨ä¾‹
    question = "How long is my daily commute to work?"
    gold_answer = "45 minutes each way"
    predicted_answer = "Your daily commute takes approximately 45 minutes in each direction."
    
    # æµ‹è¯•LLMè¯„ä¼°
    result = llm_grader(llm_client, question, gold_answer, predicted_answer)
    print(f"LLMè¯„ä¼°ç»“æœ: {result}")
    
    # æµ‹è¯•ç»¼åˆè¯„ä¼°
    comprehensive_result = calculate_comprehensive_scores(
        gold_answer=gold_answer,
        response=predicted_answer,
        question=question,
        llm_client=llm_client
    )
    print(f"\nç»¼åˆè¯„ä¼°ç»“æœ:")
    print(json.dumps(comprehensive_result, indent=2, ensure_ascii=False))
    
    # æ¸…ç†ç¼“å­˜
    cleanup_evaluation_models()
    print("\nâœ… æµ‹è¯•å®Œæˆ")