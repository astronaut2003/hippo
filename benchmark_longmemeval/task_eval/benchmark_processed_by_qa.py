"""
LongMemEval Benchmark æµ‹è¯•è„šæœ¬ï¼ˆProcessed ç‰ˆæœ¬ï¼‰

ä½¿ç”¨ mem0 è®°å¿†ç³»ç»Ÿå’Œ LLM è¿›è¡Œé—®ç­”è¯„ä¼°
é€æ¡ä¿å­˜ç»“æœåˆ° benchmark_results/QA_X/ ç›®å½•

ã€ä¸ unprocessed ç‰ˆæœ¬çš„åŒºåˆ«ã€‘ï¼š
- é»˜è®¤ infer=Trueï¼Œå¯ç”¨ mem0 çš„è®°å¿†æ¨ç†åŠŸèƒ½ï¼ˆæå–å’Œæ›´æ–°è®°å¿†ï¼‰
- mem0 ä¼šå¯¹å¯¹è¯å†å²è¿›è¡Œå¤„ç†ï¼Œæå–å…³é”®ä¿¡æ¯å½¢æˆç»“æ„åŒ–è®°å¿†
- é€‚ç”¨äºéœ€è¦è®°å¿†å‹ç¼©å’Œæ¨ç†çš„åœºæ™¯
"""

import json
import os
import sys
import time
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import logging
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

# ğŸ”¥ ä½¿ç”¨ processed ç‰ˆæœ¬çš„åŠ è½½å™¨
from task_eval.load_dataset_processed import LongMemEvalLoader, load_dataset
from task_eval.llm_client import LLMClient
from task_eval.evaluation import calculate_comprehensive_scores

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LongMemEvalBenchmark:
    """
    LongMemEval åŸºå‡†æµ‹è¯•ç±»ï¼ˆProcessed ç‰ˆæœ¬ï¼‰
    
    ã€ä¸ unprocessed ç‰ˆæœ¬çš„åŒºåˆ«ã€‘ï¼š
    - é»˜è®¤ infer=Trueï¼Œå¯ç”¨ mem0 çš„è®°å¿†æ¨ç†åŠŸèƒ½
    - mem0 ä¼šå¯¹å¯¹è¯å†å²è¿›è¡Œå¤„ç†ï¼Œæå–å…³é”®ä¿¡æ¯å½¢æˆç»“æ„åŒ–è®°å¿†
    """
    
    def __init__(
        self,
        dataset_path: str,
        gen_llm_model: str = "gpt-4o-mini-closeai",
        eval_llm_model: str = "gpt-4o-mini-closeai",
        user_id_base: str = "benchmark_processed",
        infer: bool = True,  # ğŸ”¥ é»˜è®¤ Trueï¼Œå¯ç”¨è®°å¿†æ¨ç†
        output_dir: str = "benchmark_results_processed"
    ):
        """
        åˆå§‹åŒ– Benchmark
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„
            gen_llm_model: ç”Ÿæˆç­”æ¡ˆçš„ LLM æ¨¡å‹åç§°
            eval_llm_model: è¯„ä¼°ç­”æ¡ˆçš„ LLM æ¨¡å‹åç§°
            user_id_base: user_id åŸºç¡€åç§°
            infer: æ˜¯å¦å¯ç”¨ mem0 çš„æ¨ç†åŠŸèƒ½ï¼ˆé»˜è®¤ Trueï¼‰
            output_dir: è¾“å‡ºç›®å½•
        """
        self.dataset_path = dataset_path
        self.gen_llm_model = gen_llm_model
        self.eval_llm_model = eval_llm_model
        self.user_id_base = user_id_base
        self.infer = infer
        self.output_dir = Path(output_dir)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–åŠ è½½å™¨ï¼ˆä½¿ç”¨ processed ç‰ˆæœ¬ï¼‰
        logger.info("åˆå§‹åŒ– LongMemEval åŠ è½½å™¨ï¼ˆProcessed ç‰ˆæœ¬ï¼‰...")
        self.loader = LongMemEvalLoader()
        
        # åˆå§‹åŒ–ç”Ÿæˆ LLM å®¢æˆ·ç«¯
        logger.info(f"åˆå§‹åŒ–ç”Ÿæˆ LLM å®¢æˆ·ç«¯: {gen_llm_model}")
        self.gen_llm_client = LLMClient(model_name=gen_llm_model)
        
        # åˆå§‹åŒ–è¯„ä¼° LLM å®¢æˆ·ç«¯ï¼ˆå¦‚æœä¸ç”Ÿæˆ LLM ç›¸åŒåˆ™å¤ç”¨ï¼‰
        if eval_llm_model == gen_llm_model:
            logger.info(f"è¯„ä¼° LLM ä¸ç”Ÿæˆ LLM ç›¸åŒï¼Œå¤ç”¨å®¢æˆ·ç«¯")
            self.eval_llm_client = self.gen_llm_client
        else:
            logger.info(f"åˆå§‹åŒ–è¯„ä¼° LLM å®¢æˆ·ç«¯: {eval_llm_model}")
            self.eval_llm_client = LLMClient(model_name=eval_llm_model)
        
        logger.info(f"Benchmark åˆå§‹åŒ–å®Œæˆ (infer={self.infer})")
    
    def format_memories_for_prompt(self, memories: List[Dict[str, Any]]) -> str:
        """
        å°†æ£€ç´¢åˆ°çš„è®°å¿†æ ¼å¼åŒ–ä¸º LLM prompt
        
        Args:
            memories: è®°å¿†åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–åçš„è®°å¿†æ–‡æœ¬
        """
        if not memories:
            return "No relevant memories found."
        
        formatted_parts = []
        for i, mem in enumerate(memories, 1):
            memory_text = mem.get('memory', '')
            score = mem.get('score', 0)
            rerank_score = mem.get('rerank_score', 0)
            
            formatted_parts.append(
                f"Memory {i} (relevance: {score:.3f}, rerank: {rerank_score:.3f}):\n{memory_text}"
            )
        
        return "\n\n".join(formatted_parts)
    
    def create_qa_prompt(
        self,
        question: str,
        memories: List[Dict[str, Any]],
        question_type: str = "unknown"
    ) -> str:
        """
        åˆ›å»ºé—®ç­” prompt
        
        Args:
            question: é—®é¢˜
            memories: æ£€ç´¢åˆ°çš„è®°å¿†
            question_type: é—®é¢˜ç±»å‹
            
        Returns:
            å®Œæ•´çš„ prompt
        """
        memories_text = self.format_memories_for_prompt(memories)
        
        prompt = f"""You are a helpful assistant with access to the user's conversation history and memories.

        Based on the following retrieved memories, please answer the user's question accurately and concisely.

        Question Type: {question_type}

        Retrieved Memories:
        {memories_text}

        User Question: {question}

        Instructions:
        - Answer based ONLY on the information provided in the memories above
        - If the memories don't contain enough information, say "I don't have enough information to answer this question"
        - Be concise and direct
        - For temporal questions, pay attention to dates and chronological order
        - For preference questions, focus on user's stated preferences
        - DO NOT make up information

        Answer:"""
        
        return prompt
    
    def save_sample_results(
        self,
        qa_index: int,
        score_data: Dict[str, Any],
        retrieval_data: Dict[str, Any]
    ):
        """
        ä¿å­˜å•ä¸ªæ ·æœ¬çš„ç»“æœåˆ°ç‹¬ç«‹ç›®å½•
        
        ç›®å½•ç»“æ„:
        benchmark_results_processed/
            QA_0/
                score.json      # è¯„åˆ†ç»“æœ
                retrieval.json  # æ£€ç´¢ç»“æœ
            QA_1/
                score.json
                retrieval.json
            ...
        
        Args:
            qa_index: QA æ ·æœ¬ç´¢å¼•
            score_data: è¯„åˆ†æ•°æ®
            retrieval_data: æ£€ç´¢æ•°æ®
        """
        # åˆ›å»º QA ç›®å½•
        qa_dir = self.output_dir / f"QA_{qa_index}"
        qa_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è¯„åˆ†ç»“æœ
        score_file = qa_dir / "score.json"
        with open(score_file, 'w', encoding='utf-8') as f:
            json.dump(score_data, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ£€ç´¢ç»“æœ
        retrieval_file = qa_dir / "retrieval.json"
        with open(retrieval_file, 'w', encoding='utf-8') as f:
            json.dump(retrieval_data, f, indent=2, ensure_ascii=False)
        
        logger.debug(f"[QA_{qa_index}] ç»“æœå·²ä¿å­˜åˆ° {qa_dir}")
    
    def process_single_sample(
        self,
        sample: Dict[str, Any],
        sample_idx: int,
        query_top_k: int = 5,
        save_immediately: bool = True
    ) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ ·æœ¬
        
        Args:
            sample: æ ·æœ¬æ•°æ®
            sample_idx: æ ·æœ¬ç´¢å¼•
            query_top_k: æ£€ç´¢è¿”å›çš„è®°å¿†æ•°é‡
            save_immediately: æ˜¯å¦ç«‹å³ä¿å­˜ç»“æœ
            
        Returns:
            å¤„ç†ç»“æœ
        """
        # è®°å½•æ•´ä½“å¼€å§‹æ—¶é—´
        qa_start_time = time.time()
        
        question_id = sample.get('question_id', 'unknown')
        question = sample.get('question', '')
        question_type = sample.get('question_type', 'unknown')
        gold_answer = sample.get('answer', '')
        question_date = sample.get('question_date', '')
        
        logger.info(f"\n{'='*80}")
        logger.info(f"[QA_{sample_idx}] {question_id} (Processed Mode, infer={self.infer})")
        logger.info(f"é—®é¢˜ç±»å‹: {question_type}")
        logger.info(f"é—®é¢˜: {question}")
        logger.info(f"æ ‡å‡†ç­”æ¡ˆ: {gold_answer}")
        logger.info(f"{'='*80}")
        
        result = {
            'sample_idx': sample_idx,
            'question_id': question_id,
            'question': question,
            'question_type': question_type,
            'gold_answer': gold_answer,
            'question_date': question_date,
            'user_id': f"{self.user_id_base}_{sample_idx}",
            'infer_mode': self.infer,  # ğŸ”¥ è®°å½•æ¨ç†æ¨¡å¼
            'status': 'success',
            'timestamp': datetime.now().isoformat()
        }
        
        # åˆå§‹åŒ–ä¿å­˜æ•°æ®ç»“æ„
        score_data = {
            'sample_idx': sample_idx,
            'question_id': question_id,
            'question': question,
            'question_type': question_type,
            'gold_answer': gold_answer,
            'question_date': question_date,
            'gen_llm_model': self.gen_llm_model,
            'eval_llm_model': self.eval_llm_model,
            'infer_mode': self.infer,  # ğŸ”¥ è®°å½•æ¨ç†æ¨¡å¼
            'timestamp': datetime.now().isoformat()
        }
        
        retrieval_data = {
            'sample_idx': sample_idx,
            'question_id': question_id,
            'question': question,
            'query_top_k': query_top_k,
            'infer_mode': self.infer,  # ğŸ”¥ è®°å½•æ¨ç†æ¨¡å¼
            'timestamp': datetime.now().isoformat()
        }
        
        # åˆå§‹åŒ–æ—¶é—´ç»Ÿè®¡
        timing_info = {
            'load_time': 0.0,
            'retrieval_time': 0.0,
            'generation_time': 0.0,
            'evaluation_time': 0.0,
            'cleanup_time': 0.0,
            'total_time': 0.0
        }
        
        try:
            # 1. åŠ è½½å¯¹è¯å†å²åˆ°è®°å¿†ç³»ç»Ÿï¼ˆğŸ”¥ ä½¿ç”¨ infer=True è¿›è¡Œé¢„å¤„ç†ï¼‰
            logger.info(f"[QA_{sample_idx}] åŠ è½½å¯¹è¯å†å²å¹¶è¿›è¡Œè®°å¿†é¢„å¤„ç† (infer={self.infer})...")
            load_start = time.time()
            
            load_result = self.loader.load_sample(
                sample=sample,
                sample_idx=sample_idx,
                user_id_base=self.user_id_base,
                infer=self.infer,  # ğŸ”¥ é»˜è®¤ Trueï¼Œè¿›è¡Œè®°å¿†é¢„å¤„ç†
                clean_before_add=True
            )
            
            load_end = time.time()
            timing_info['load_time'] = round(load_end - load_start, 4)
            
            load_info = {
                'total_sessions': load_result['add_result']['total_sessions'],
                'added_sessions': load_result['add_result']['added_sessions'],
                'failed_sessions': load_result['add_result']['failed_sessions'],
                'infer_mode': load_result['add_result'].get('infer_mode', self.infer)
            }
            
            result['load_result'] = load_info
            retrieval_data['load_result'] = load_info
            
            logger.info(
                f"[QA_{sample_idx}] åŠ è½½å®Œæˆ: "
                f"{load_info['added_sessions']}/{load_info['total_sessions']} ä¸ªä¼šè¯ "
                f"(infer={self.infer}, è€—æ—¶: {timing_info['load_time']:.2f}s)"
            )
            
            # 2. æ£€ç´¢ç›¸å…³è®°å¿†
            logger.info(f"[QA_{sample_idx}] æ£€ç´¢ç›¸å…³è®°å¿†...")
            retrieval_start = time.time()
            
            memories = self.loader.search_sample(
                question=question,
                sample_idx=sample_idx,
                user_id_base=self.user_id_base,
                query_top_k=query_top_k
            )
            
            retrieval_end = time.time()
            timing_info['retrieval_time'] = round(retrieval_end - retrieval_start, 4)
            
            result['retrieved_memories_count'] = len(memories)
            result['retrieved_memories'] = memories
            
            # ä¿å­˜æ£€ç´¢ç»“æœ
            retrieval_data['retrieved_memories_count'] = len(memories)
            retrieval_data['retrieved_memories'] = memories
            
            logger.info(
                f"[QA_{sample_idx}] æ£€ç´¢åˆ° {len(memories)} æ¡è®°å¿† "
                f"(è€—æ—¶: {timing_info['retrieval_time']:.2f}s)"
            )
            
            # 3. ä½¿ç”¨ç”Ÿæˆ LLM ç”Ÿæˆç­”æ¡ˆ
            logger.info(f"[QA_{sample_idx}] ä½¿ç”¨ç”Ÿæˆ LLM ({self.gen_llm_model}) ç”Ÿæˆç­”æ¡ˆ...")
            generation_start = time.time()
            
            prompt = self.create_qa_prompt(question, memories, question_type)
            
            # è®¡ç®—ç”Ÿæˆ prompt çš„ token æ•°é‡
            gen_prompt_tokens = self.gen_llm_client.count_tokens(prompt)
            gen_context_info = self.gen_llm_client.get_context_info()
            
            predicted_answer = self.gen_llm_client.generate_answer(
                prompt=prompt,
                temperature=0.1,
                max_tokens=512
            )
            
            generation_end = time.time()
            timing_info['generation_time'] = round(generation_end - generation_start, 4)
            
            # è®¡ç®—ç”Ÿæˆç­”æ¡ˆçš„ token æ•°é‡
            gen_answer_tokens = self.gen_llm_client.count_tokens(predicted_answer)
            
            # ç”Ÿæˆé˜¶æ®µçš„ token ä½¿ç”¨ä¿¡æ¯
            gen_token_usage = {
                'prompt_tokens': gen_prompt_tokens,
                'answer_tokens': gen_answer_tokens,
                'total_tokens': gen_prompt_tokens + gen_answer_tokens,
                'context_length': gen_context_info.get('context_length', 0),
                'max_context_tokens': gen_context_info.get('max_context_tokens', 0),
                'prompt_ratio': round(gen_prompt_tokens / gen_context_info.get('context_length', 1) * 100, 2),
                'tokenizer_type': 'tiktoken' if gen_context_info.get('tokenizer_available') else 'estimated',
                'encoding': gen_context_info.get('encoding', 'unknown')
            }
            
            result['predicted_answer'] = predicted_answer
            result['prompt_length'] = len(prompt)
            result['gen_token_usage'] = gen_token_usage
            
            score_data['predicted_answer'] = predicted_answer
            score_data['prompt_length'] = len(prompt)
            score_data['gen_token_usage'] = gen_token_usage
            
            logger.info(f"[QA_{sample_idx}] é¢„æµ‹ç­”æ¡ˆ: {predicted_answer}")
            logger.info(
                f"[QA_{sample_idx}] ç”Ÿæˆ Token ä½¿ç”¨: "
                f"prompt={gen_prompt_tokens}, answer={gen_answer_tokens}, total={gen_prompt_tokens + gen_answer_tokens} "
                f"(è€—æ—¶: {timing_info['generation_time']:.2f}s)"
            )
            
            # 4. è¯„ä¼°ç­”æ¡ˆè´¨é‡ï¼ˆä½¿ç”¨è¯„ä¼° LLM è¿›è¡Œ LLM Judgeï¼‰
            logger.info(f"[QA_{sample_idx}] ä½¿ç”¨è¯„ä¼° LLM ({self.eval_llm_model}) è¯„ä¼°ç­”æ¡ˆè´¨é‡...")
            evaluation_start = time.time()
            
            try:
                eval_scores = calculate_comprehensive_scores(
                    gold_answer=gold_answer,
                    response=predicted_answer,
                    question=question,
                    question_type=question_type,
                    llm_client=self.eval_llm_client,  # ä½¿ç”¨è¯„ä¼° LLM
                    metrics=['exact_match', 'f1', 'rouge', 'semantic_similarity', 'llm_judge']
                )
                
                result['evaluation'] = eval_scores
                score_data['evaluation'] = eval_scores
                score_data['scores'] = eval_scores.get('scores', {})
                
                f1_score = eval_scores.get('scores', {}).get('token_f1', 0)
                llm_accuracy = eval_scores.get('scores', {}).get('llm_accuracy', 0)
                logger.info(f"[QA_{sample_idx}] F1åˆ†æ•°: {f1_score:.3f}, LLM Judge: {llm_accuracy}")
                
            except Exception as eval_error:
                logger.warning(f"[QA_{sample_idx}] è¯„ä¼°å¤±è´¥: {eval_error}")
                result['evaluation'] = {'error': str(eval_error)}
                score_data['evaluation'] = {'error': str(eval_error)}
                score_data['scores'] = {}
            
            evaluation_end = time.time()
            timing_info['evaluation_time'] = round(evaluation_end - evaluation_start, 4)
            logger.info(f"[QA_{sample_idx}] è¯„ä¼°è€—æ—¶: {timing_info['evaluation_time']:.2f}s")
            
            # 5. æ¸…ç†è®°å¿†
            cleanup_start = time.time()
            self.loader.reset_memory(sample_idx=sample_idx, user_id_base=self.user_id_base)
            cleanup_end = time.time()
            timing_info['cleanup_time'] = round(cleanup_end - cleanup_start, 4)
            
            result['status'] = 'success'
            score_data['status'] = 'success'
            retrieval_data['status'] = 'success'
            
        except Exception as e:
            logger.error(f"[QA_{sample_idx}] å¤„ç†å¤±è´¥: {e}", exc_info=True)
            result['status'] = 'failed'
            result['error'] = str(e)
            
            score_data['status'] = 'failed'
            score_data['error'] = str(e)
            
            retrieval_data['status'] = 'failed'
            retrieval_data['error'] = str(e)
        
        # è®¡ç®—æ€»æ—¶é—´
        qa_end_time = time.time()
        timing_info['total_time'] = round(qa_end_time - qa_start_time, 4)
        
        # æ·»åŠ æ—¶é—´ç»Ÿè®¡åˆ°ç»“æœä¸­
        result['timing'] = timing_info
        score_data['timing'] = timing_info
        retrieval_data['timing'] = {
            'retrieval_time': timing_info['retrieval_time'],
            'load_time': timing_info['load_time']
        }
        
        logger.info(
            f"[QA_{sample_idx}] æ—¶é—´ç»Ÿè®¡: "
            f"åŠ è½½={timing_info['load_time']:.2f}s, "
            f"æ£€ç´¢={timing_info['retrieval_time']:.2f}s, "
            f"ç”Ÿæˆ={timing_info['generation_time']:.2f}s, "
            f"è¯„ä¼°={timing_info['evaluation_time']:.2f}s, "
            f"æ€»è®¡={timing_info['total_time']:.2f}s"
        )
        
        # ç«‹å³ä¿å­˜ç»“æœ
        if save_immediately:
            self.save_sample_results(sample_idx, score_data, retrieval_data)
        
        return result
    
    def run_benchmark(
        self,
        start_index: Optional[int] = None,
        end_index: Optional[int] = None,
        query_top_k: int = 5,
        save_summary: bool = True
    ) -> Dict[str, Any]:
        """
        è¿è¡ŒåŸºå‡†æµ‹è¯•
        
        Args:
            start_index: å¼€å§‹çš„ QA ç´¢å¼•ï¼ˆåŒ…å«ï¼‰ï¼ŒNone è¡¨ç¤ºä»å¤´å¼€å§‹
            end_index: ç»“æŸçš„ QA ç´¢å¼•ï¼ˆåŒ…å«ï¼‰ï¼ŒNone è¡¨ç¤ºåˆ°æœ«å°¾
            query_top_k: æ£€ç´¢è¿”å›çš„è®°å¿†æ•°é‡
            save_summary: æ˜¯å¦ä¿å­˜æ±‡æ€»ç»“æœ
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        logger.info("="*80)
        logger.info("å¼€å§‹ LongMemEval Benchmark æµ‹è¯•ï¼ˆProcessed ç‰ˆæœ¬ï¼‰")
        logger.info("="*80)
        logger.info(f"æ•°æ®é›†: {self.dataset_path}")
        logger.info(f"ç”Ÿæˆ LLM æ¨¡å‹: {self.gen_llm_model}")
        logger.info(f"è¯„ä¼° LLM æ¨¡å‹: {self.eval_llm_model}")
        logger.info(f"ğŸ”¥ Infer æ¨¡å¼: {self.infer} (è®°å¿†é¢„å¤„ç†)")
        logger.info(f"æ£€ç´¢ Top-K: {query_top_k}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # åŠ è½½æ•°æ®é›†ï¼ˆå…ˆåŠ è½½å…¨éƒ¨ï¼Œå†æ ¹æ®ç´¢å¼•ç­›é€‰ï¼‰
        logger.info("\nåŠ è½½æ•°æ®é›†...")
        all_samples = load_dataset(self.dataset_path)
        total_samples_in_dataset = len(all_samples)
        logger.info(f"æ•°æ®é›†å…± {total_samples_in_dataset} ä¸ªæ ·æœ¬")
        
        # ç¡®å®šç´¢å¼•èŒƒå›´
        if start_index is None:
            start_index = 0
        if end_index is None:
            end_index = total_samples_in_dataset - 1
        
        # éªŒè¯ç´¢å¼•èŒƒå›´
        start_index = max(0, start_index)
        end_index = min(total_samples_in_dataset - 1, end_index)
        
        if start_index > end_index:
            raise ValueError(f"æ— æ•ˆçš„ç´¢å¼•èŒƒå›´: start_index={start_index}, end_index={end_index}")
        
        logger.info(f"å¤„ç†èŒƒå›´: QA_{start_index} åˆ° QA_{end_index}")
        
        # ç­›é€‰æ ·æœ¬
        samples_to_process = []
        indices_to_process = []
        
        for idx in range(start_index, end_index + 1):
            sample = all_samples[idx]
            # ä½¿ç”¨æ ·æœ¬è‡ªå¸¦çš„ç´¢å¼•ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åˆ—è¡¨ç´¢å¼•
            original_idx = sample.get('sample_index', idx)
            samples_to_process.append(sample)
            indices_to_process.append(original_idx)
        
        logger.info(f"å°†å¤„ç† {len(samples_to_process)} ä¸ªæ ·æœ¬")
        
        # å¤„ç†æ¯ä¸ªæ ·æœ¬
        all_results = []
        start_time = datetime.now()
        
        for i, (sample, sample_idx) in enumerate(tqdm(
            zip(samples_to_process, indices_to_process),
            total=len(samples_to_process),
            desc="å¤„ç†æ ·æœ¬ (Processed)"
        )):
            logger.info(f"\nå¤„ç† {i+1}/{len(samples_to_process)} (QA_{sample_idx})")
            
            result = self.process_single_sample(
                sample=sample,
                sample_idx=sample_idx,
                query_top_k=query_top_k,
                save_immediately=True
            )
            
            all_results.append(result)
        
        end_time = datetime.now()
        total_time = (end_time - start_time).total_seconds()
        
        # ç»Ÿè®¡ç»“æœ
        logger.info("\n" + "="*80)
        logger.info("æµ‹è¯•å®Œæˆï¼Œç»Ÿè®¡ç»“æœ...")
        logger.info("="*80)
        
        successful = [r for r in all_results if r['status'] == 'success']
        failed = [r for r in all_results if r['status'] == 'failed']
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = self._calculate_average_metrics(successful)
        
        summary = {
            'benchmark_info': {
                'dataset_path': self.dataset_path,
                'gen_llm_model': self.gen_llm_model,
                'eval_llm_model': self.eval_llm_model,
                'user_id_base': self.user_id_base,
                'infer': self.infer,
                'mode': 'processed',  # ğŸ”¥ æ ‡è®°ä¸º processed æ¨¡å¼
                'query_top_k': query_top_k,
                'start_index': start_index,
                'end_index': end_index,
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'total_time_seconds': total_time
            },
            'statistics': {
                'total_samples_in_dataset': total_samples_in_dataset,
                'samples_processed': len(samples_to_process),
                'successful': len(successful),
                'failed': len(failed),
                'success_rate': len(successful) / len(samples_to_process) if samples_to_process else 0,
                'avg_time_per_sample': total_time / len(samples_to_process) if samples_to_process else 0
            },
            'average_metrics': avg_metrics,
            'processed_indices': indices_to_process,
            'failed_indices': [r['sample_idx'] for r in failed]
        }
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(summary)
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        if save_summary:
            self._save_summary(summary)
        
        return summary
    
    def _calculate_average_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        if not results:
            return {}
        
        metrics = {}
        metric_names = ['exact_match', 'f1', 'semantic_similarity', 'rouge_1', 'rouge_l']
        
        for metric_name in metric_names:
            values = []
            for r in results:
                if 'evaluation' in r and 'scores' in r['evaluation']:
                    scores = r['evaluation']['scores']
                    
                    # å¤„ç† rouge åµŒå¥—ç»“æ„
                    if metric_name.startswith('rouge_'):
                        rouge_key = metric_name.replace('rouge_', 'rouge-')
                        if 'rouge' in scores and rouge_key in scores.get('rouge', {}):
                            value = scores['rouge'][rouge_key]
                            if value is not None:
                                values.append(value)
                    else:
                        value = scores.get(metric_name)
                        if value is not None:
                            values.append(value)
            
            if values:
                metrics[f'avg_{metric_name}'] = sum(values) / len(values)
        
        # è®¡ç®—å¹³å‡æ£€ç´¢è®°å¿†æ•°
        memory_counts = [r.get('retrieved_memories_count', 0) for r in results]
        if memory_counts:
            metrics['avg_retrieved_memories'] = sum(memory_counts) / len(memory_counts)
        
        return metrics
    
    def _print_summary(self, summary: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        stats = summary['statistics']
        metrics = summary['average_metrics']
        info = summary['benchmark_info']
        
        print("\n" + "="*80)
        print("ğŸ“Š æµ‹è¯•æ‘˜è¦ï¼ˆProcessed æ¨¡å¼ï¼‰")
        print("="*80)
        print(f"å¤„ç†èŒƒå›´: QA_{info['start_index']} ~ QA_{info['end_index']}")
        print(f"ğŸ”¥ Infer æ¨¡å¼: {info['infer']} (è®°å¿†é¢„å¤„ç†)")
        print(f"æ€»æ ·æœ¬æ•°: {stats['samples_processed']}")
        print(f"æˆåŠŸ: {stats['successful']} | å¤±è´¥: {stats['failed']}")
        print(f"æˆåŠŸç‡: {stats['success_rate']:.2%}")
        print(f"æ€»è€—æ—¶: {stats['avg_time_per_sample']*stats['samples_processed']:.2f}ç§’")
        print(f"å¹³å‡è€—æ—¶: {stats['avg_time_per_sample']:.2f}ç§’/æ ·æœ¬")
        
        if summary['failed_indices']:
            print(f"\nâŒ å¤±è´¥çš„æ ·æœ¬: {summary['failed_indices']}")
        
        print("\nğŸ“ˆ å¹³å‡æŒ‡æ ‡:")
        for metric_name, value in metrics.items():
            if metric_name.startswith('avg_'):
                display_name = metric_name.replace('avg_', '').replace('_', ' ').title()
                if 'memories' in metric_name:
                    print(f"  {display_name}: {value:.1f}")
                else:
                    print(f"  {display_name}: {value:.4f}")
        
        print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        print("="*80)
    
    def _save_summary(self, summary: Dict[str, Any]):
        """ä¿å­˜æ±‡æ€»ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        gen_model_name = self.gen_llm_model.replace(':', '_').replace('/', '_')
        eval_model_name = self.eval_llm_model.replace(':', '_').replace('/', '_')
        
        start_idx = summary['benchmark_info']['start_index']
        end_idx = summary['benchmark_info']['end_index']
        
        # ä¿å­˜æ±‡æ€»æ–‡ä»¶ï¼ˆæ·»åŠ  processed æ ‡è®°ï¼‰
        summary_file = self.output_dir / f"summary_processed_gen_{gen_model_name}_eval_{eval_model_name}_QA{start_idx}-{end_idx}_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nğŸ’¾ æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")
    
    def check_existing_results(self, start_index: int, end_index: int) -> List[int]:
        """
        æ£€æŸ¥å·²å­˜åœ¨çš„ç»“æœ
        
        Args:
            start_index: å¼€å§‹ç´¢å¼•
            end_index: ç»“æŸç´¢å¼•
            
        Returns:
            å·²å®Œæˆçš„ç´¢å¼•åˆ—è¡¨
        """
        completed = []
        for idx in range(start_index, end_index + 1):
            qa_dir = self.output_dir / f"QA_{idx}"
            score_file = qa_dir / "score.json"
            retrieval_file = qa_dir / "retrieval.json"
            
            if score_file.exists() and retrieval_file.exists():
                completed.append(idx)
        
        return completed
    
    def run_benchmark_resume(
        self,
        start_index: Optional[int] = None,
        end_index: Optional[int] = None,
        query_top_k: int = 5,
        skip_existing: bool = True,
        save_summary: bool = True
    ) -> Dict[str, Any]:
        """
        è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        
        Args:
            start_index: å¼€å§‹çš„ QA ç´¢å¼•ï¼ˆåŒ…å«ï¼‰ï¼ŒNone è¡¨ç¤ºä»å¤´å¼€å§‹
            end_index: ç»“æŸçš„ QA ç´¢å¼•ï¼ˆåŒ…å«ï¼‰ï¼ŒNone è¡¨ç¤ºåˆ°æœ«å°¾
            query_top_k: æ£€ç´¢è¿”å›çš„è®°å¿†æ•°é‡
            skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„ç»“æœ
            save_summary: æ˜¯å¦ä¿å­˜æ±‡æ€»ç»“æœ
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        logger.info("="*80)
        logger.info("å¼€å§‹ LongMemEval Benchmark æµ‹è¯•ï¼ˆProcessed ç‰ˆæœ¬ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰")
        logger.info("="*80)
        logger.info(f"ç”Ÿæˆ LLM æ¨¡å‹: {self.gen_llm_model}")
        logger.info(f"è¯„ä¼° LLM æ¨¡å‹: {self.eval_llm_model}")
        logger.info(f"ğŸ”¥ Infer æ¨¡å¼: {self.infer} (è®°å¿†é¢„å¤„ç†)")
        
        # åŠ è½½æ•°æ®é›†
        all_samples = load_dataset(self.dataset_path)
        total_samples_in_dataset = len(all_samples)
        
        # ç¡®å®šç´¢å¼•èŒƒå›´
        if start_index is None:
            start_index = 0
        if end_index is None:
            end_index = total_samples_in_dataset - 1
        
        start_index = max(0, start_index)
        end_index = min(total_samples_in_dataset - 1, end_index)
        
        # æ£€æŸ¥å·²å®Œæˆçš„ç»“æœ
        if skip_existing:
            completed = self.check_existing_results(start_index, end_index)
            if completed:
                logger.info(f"å‘ç° {len(completed)} ä¸ªå·²å®Œæˆçš„æ ·æœ¬ï¼Œå°†è·³è¿‡")
        else:
            completed = []
        
        # ç­›é€‰éœ€è¦å¤„ç†çš„æ ·æœ¬
        samples_to_process = []
        indices_to_process = []
        
        for idx in range(start_index, end_index + 1):
            if idx in completed:
                continue
            sample = all_samples[idx]
            original_idx = sample.get('sample_index', idx)
            samples_to_process.append(sample)
            indices_to_process.append(original_idx)
        
        logger.info(f"å°†å¤„ç† {len(samples_to_process)} ä¸ªæ ·æœ¬ï¼ˆè·³è¿‡ {len(completed)} ä¸ªï¼‰")
        
        if not samples_to_process:
            logger.info("æ‰€æœ‰æ ·æœ¬å·²å®Œæˆï¼Œæ— éœ€å¤„ç†")
            return {'status': 'all_completed', 'completed_count': len(completed)}
        
        # å¤„ç†æ ·æœ¬
        all_results = []
        start_time = datetime.now()
        
        for i, (sample, sample_idx) in enumerate(tqdm(
            zip(samples_to_process, indices_to_process),
            total=len(samples_to_process),
            desc="å¤„ç†æ ·æœ¬ (Processed)"
        )):
            result = self.process_single_sample(
                sample=sample,
                sample_idx=sample_idx,
                query_top_k=query_top_k,
                save_immediately=True
            )
            all_results.append(result)
        
        end_time = datetime.now()
        total_time = (end_time - start_time).total_seconds()
        
        # ç»Ÿè®¡å’Œä¿å­˜
        successful = [r for r in all_results if r['status'] == 'success']
        failed = [r for r in all_results if r['status'] == 'failed']
        avg_metrics = self._calculate_average_metrics(successful)
        
        summary = {
            'benchmark_info': {
                'dataset_path': self.dataset_path,
                'gen_llm_model': self.gen_llm_model,
                'eval_llm_model': self.eval_llm_model,
                'infer': self.infer,
                'mode': 'processed',  # ğŸ”¥ æ ‡è®°ä¸º processed æ¨¡å¼
                'start_index': start_index,
                'end_index': end_index,
                'query_top_k': query_top_k,
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'total_time_seconds': total_time
            },
            'statistics': {
                'total_in_range': end_index - start_index + 1,
                'skipped': len(completed),
                'processed': len(samples_to_process),
                'successful': len(successful),
                'failed': len(failed)
            },
            'average_metrics': avg_metrics,
            'failed_indices': [r['sample_idx'] for r in failed]
        }
        
        self._print_summary(summary)
        
        if save_summary:
            self._save_summary(summary)
        
        return summary


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='LongMemEval Benchmark æµ‹è¯•ï¼ˆProcessed ç‰ˆæœ¬ï¼Œé€æ¡ä¿å­˜ï¼‰')
    parser.add_argument(
        '--dataset',
        type=str,
        default='benchmark_longmemeval/dataset/LongMemEval/longmemeval_s_cleaned.json',
        help='æ•°æ®é›†è·¯å¾„'
    )
    parser.add_argument(
        '--gen-model',
        type=str,
        default='gpt-4o-mini-closeai',
        help='ç”Ÿæˆç­”æ¡ˆçš„ LLM æ¨¡å‹åç§°'
    )
    parser.add_argument(
        '--eval-model',
        type=str,
        default='gpt-4o-mini-closeai',
        help='è¯„ä¼°ç­”æ¡ˆçš„ LLM æ¨¡å‹åç§°'
    )
    parser.add_argument(
        '--start',
        type=int,
        default=None,
        help='å¼€å§‹çš„ QA ç´¢å¼•ï¼ˆåŒ…å«ï¼‰ï¼Œä¸æŒ‡å®šåˆ™ä»0å¼€å§‹'
    )
    parser.add_argument(
        '--end',
        type=int,
        default=None,
        help='ç»“æŸçš„ QA ç´¢å¼•ï¼ˆåŒ…å«ï¼‰ï¼Œä¸æŒ‡å®šåˆ™åˆ°æœ«å°¾'
    )
    parser.add_argument(
        '--top-k',
        type=int,
        default=20,
        help='æ£€ç´¢è¿”å›çš„è®°å¿†æ•°é‡'
    )
    parser.add_argument(
        '--no-infer',
        action='store_true',
        help='ç¦ç”¨ mem0 çš„æ¨ç†åŠŸèƒ½ï¼ˆé»˜è®¤å¯ç”¨ï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='benchmark_longmemeval/benchmark_results_processed',
        help='è¾“å‡ºç›®å½•'
    )
    parser.add_argument(
        '--resume',
        action='store_true',
        help='æ–­ç‚¹ç»­ä¼ æ¨¡å¼ï¼Œè·³è¿‡å·²å®Œæˆçš„æ ·æœ¬'
    )
    parser.add_argument(
        '--no-skip',
        action='store_true',
        help='ä¸è·³è¿‡å·²å­˜åœ¨çš„ç»“æœï¼ˆè¦†ç›–æ¨¡å¼ï¼‰'
    )
    
    args = parser.parse_args()
    
    # ğŸ”¥ é»˜è®¤ infer=Trueï¼Œä½¿ç”¨ --no-infer å¯ä»¥ç¦ç”¨
    infer = not args.no_infer
    
    # åˆ›å»º benchmark å®ä¾‹
    benchmark = LongMemEvalBenchmark(
        dataset_path=args.dataset,
        gen_llm_model=args.gen_model,
        eval_llm_model=args.eval_model,
        user_id_base='benchmark_processed',
        infer=infer,  # ğŸ”¥ é»˜è®¤ True
        output_dir=args.output_dir
    )
    
    # è¿è¡Œæµ‹è¯•
    if args.resume:
        results = benchmark.run_benchmark_resume(
            start_index=args.start,
            end_index=args.end,
            query_top_k=args.top_k,
            skip_existing=not args.no_skip,
            save_summary=True
        )
    else:
        results = benchmark.run_benchmark(
            start_index=args.start,
            end_index=args.end,
            query_top_k=args.top_k,
            save_summary=True
        )
    
    return results


if __name__ == "__main__":
    main()