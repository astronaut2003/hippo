import os
import logging
import traceback
from typing import List, Dict, Any, Optional
from openai import OpenAI
import re

# æ·»åŠ tiktokenæ”¯æŒ
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktokenæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•ä¼°ç®—æ–¹æ³•ã€‚å»ºè®®å®‰è£…: pip install tiktoken")


class LLMClient:
    """ç»Ÿä¸€çš„å¤§æ¨¡å‹å®¢æˆ·ç«¯æ¥å£ï¼Œæ”¯æŒDeepSeekã€GPTã€GPT-CloseAIç­‰å¤šä¸ªæä¾›å•†"""
    
    # APIæä¾›å•†é…ç½®
    API_PROVIDERS = {
        "deepseek": {
            "base_url": "https://api.deepseek.com",
            "api_key_env": "DEEPSEEK_API_KEY",
            "supports_json_format": True
        },
        "cstcloud":{
            "base_url": "https://uni-api.cstcloud.cn/v1",
            "api_key_env": "CSTCLOUD_API_KEY",
            "supports_json_format": True
        },
        "openai": {
            "base_url": "https://api.openai.com/v1",
            "api_key_env": "OPENAI_API_KEY",
            "supports_json_format": True
        },
        "openai-proxy": {  # closeaiä¸­è½¬
            "base_url": "https://api.openai-proxy.org/v1",
            "api_key_env": "CLOSEAI_API_KEY",  # ç‹¬ç«‹çš„ç¯å¢ƒå˜é‡
            "fallback_env": "OPENAI_API_KEY",   # å¤‡ç”¨ç¯å¢ƒå˜é‡
            "supports_json_format": True
        }
    }
    
    # æ¨¡å‹é…ç½®ï¼ˆåŒ…å«æä¾›å•†ä¿¡æ¯ï¼‰
    MODEL_CONFIGS = {
        # DeepSeek ç³»åˆ—
        "deepseek-chat": {
            "provider": "deepseek",
            "context_length": 131072,    # 128K
            "max_output": 8192,
            "default_output": 4096,
            "encoding": "cl100k_base"
        },
        "deepseek-reasoner": {
            "provider": "deepseek",
            "context_length": 131072,    # 128K
            "max_output": 65536,
            "default_output": 32768,
            "encoding": "cl100k_base"
        },
        
        # ä¸­å›½ç§‘æŠ€äº‘å¤§æ¨¡å‹API
        "deepseek-r1:671b-0528": {
            "provider": "cstcloud",
            "context_length": 65536,
            "max_output": 8192,
            # "default_output": 4096,
            # ä¿®æ”¹é»˜è®¤è¾“å‡ºä¸º8192ï¼Œæ›´é€‚åˆæ•°æ®å¤„ç†åœºæ™¯
            "default_output": 8192,
            "encoding": "cl100k_base"
        },
        "deepseek-v3:671b": {
            "provider": "cstcloud",
            "context_length": 65536,
            "max_output": 8192,
            "default_output": 4096,
            "encoding": "cl100k_base"
        },
        
        # OpenAI GPT ç³»åˆ—
        "gpt-4o-mini": {
            "provider": "openai",
            "context_length": 131072,    # 128K
            "max_output": 16384,
            "default_output": 8192,
            "encoding": "o200k_base"
        },
        "gpt-4o": {
            "provider": "openai",
            "context_length": 131072,    # 128K
            "max_output": 16384,
            "default_output": 8192,
            "encoding": "o200k_base"
        },
        "gpt-4-turbo": {
            "provider": "openai",
            "context_length": 131072,
            "max_output": 4096,
            "default_output": 2048,
            "encoding": "o200k_base"
        },
        
        # CloseAIä¸­è½¬GPTï¼ˆä½¿ç”¨ç›¸åŒé…ç½®ï¼Œä¸åŒæä¾›å•†ï¼‰
        "gpt-4o-mini-closeai": {
            "provider": "openai-proxy",
            "context_length": 131072,
            "max_output": 16384,
            "default_output": 8192,
            "encoding": "o200k_base",
            "actual_model": "gpt-4o-mini"  # å®é™…è°ƒç”¨çš„æ¨¡å‹å
        },
        "gpt-4o-closeai": {
            "provider": "openai-proxy",
            "context_length": 131072,
            "max_output": 16384,
            "default_output": 8192,
            "encoding": "o200k_base",
            "actual_model": "gpt-4o"
        }
    }
    
    def __init__(self, 
             model_name: str = "deepseek-reasoner",
             api_key: Optional[str] = None,
             base_url: Optional[str] = None,
             max_context_ratio: float = 0.85):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            model_name: æ¨¡å‹åç§°
            api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            base_url: APIåŸºç¡€URLï¼ˆå¯é€‰ï¼Œé»˜è®¤æ ¹æ®æ¨¡å‹è‡ªåŠ¨é€‰æ‹©ï¼‰
            max_context_ratio: ä¸Šä¸‹æ–‡å æ€»é•¿åº¦çš„æ¯”ä¾‹
        """
        # 1. é¦–å…ˆåˆå§‹åŒ– loggerï¼ˆå› ä¸ºå…¶ä»–æ–¹æ³•éœ€è¦ç”¨åˆ°ï¼‰
        self.logger = logging.getLogger(__name__)
        
        # 2. è®¾ç½®åŸºæœ¬å‚æ•°
        self.model_name = model_name
        self.max_context_ratio = max_context_ratio
        
        # 3. è·å–æ¨¡å‹é…ç½®
        self.model_config = self.MODEL_CONFIGS.get(model_name)
        if not self.model_config:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}ã€‚æ”¯æŒçš„æ¨¡å‹: {list(self.MODEL_CONFIGS.keys())}")
        
        # 4. ç¡®å®šAPIæä¾›å•†
        self.provider = self.model_config["provider"]
        self.provider_config = self.API_PROVIDERS.get(self.provider)
        if not self.provider_config:
            raise ValueError(f"ä¸æ”¯æŒçš„APIæä¾›å•†: {self.provider}")
        
        # 5. è®¾ç½®APIå¯†é’¥ï¼ˆæ­¤æ—¶ logger å·²ç»åˆå§‹åŒ–ï¼Œå¯ä»¥å®‰å…¨è°ƒç”¨ _get_api_keyï¼‰
        self.api_key = api_key or self._get_api_key()
        if not self.api_key:
            env_vars = [self.provider_config["api_key_env"]]
            if "fallback_env" in self.provider_config:
                env_vars.append(self.provider_config["fallback_env"])
            raise ValueError(
                f"æœªæ‰¾åˆ°APIå¯†é’¥ã€‚è¯·è®¾ç½®ä»¥ä¸‹ä»»ä¸€ç¯å¢ƒå˜é‡: {', '.join(env_vars)} "
                f"æˆ–ä¼ å…¥api_keyå‚æ•°"
            )
        
        # 6. è®¾ç½®base_urlï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤ï¼‰
        self.base_url = base_url or self.provider_config["base_url"]
        
        # 7. è·å–å®é™…è°ƒç”¨çš„æ¨¡å‹åï¼ˆå¤„ç†closeaiç­‰ä¸­è½¬æƒ…å†µï¼‰
        self.actual_model = self.model_config.get("actual_model", model_name)
        
        # 8. è®¾ç½®ä¸Šä¸‹æ–‡å‚æ•°
        self.context_length = self.model_config["context_length"]
        self.max_output_tokens = self.model_config["max_output"]
        self.default_output_tokens = self.model_config["default_output"]
        self.max_context_tokens = int(self.context_length * max_context_ratio)
        
        # 9. æ˜¯å¦æ”¯æŒJSONæ ¼å¼
        self.supports_json_format = self.provider_config["supports_json_format"]
        
        # 10. åˆå§‹åŒ–tokenizer
        self.tokenizer = self._init_tokenizer()
        
        # 11. åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = self._init_client()
        
        # 12. è®°å½•åˆå§‹åŒ–ä¿¡æ¯
        self._log_initialization()
    
    def _get_api_key(self) -> Optional[str]:
        """
        ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        æ”¯æŒä¸»ç¯å¢ƒå˜é‡å’Œå¤‡ç”¨ç¯å¢ƒå˜é‡ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
        
        Returns:
            APIå¯†é’¥ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        # è·å–ä¸»ç¯å¢ƒå˜é‡å
        primary_env = self.provider_config["api_key_env"]
        
        # å°è¯•ä»ä¸»ç¯å¢ƒå˜é‡è·å–
        api_key = os.getenv(primary_env)
        if api_key:
            self.logger.debug(f"âœ… ä»ç¯å¢ƒå˜é‡ {primary_env} è¯»å–APIå¯†é’¥")
            return api_key
        
        # å¦‚æœæœ‰å¤‡ç”¨ç¯å¢ƒå˜é‡ï¼Œå°è¯•ä»å¤‡ç”¨ç¯å¢ƒå˜é‡è·å–
        if "fallback_env" in self.provider_config:
            fallback_env = self.provider_config["fallback_env"]
            api_key = os.getenv(fallback_env)
            if api_key:
                self.logger.info(
                    f"âš ï¸  ä¸»ç¯å¢ƒå˜é‡ {primary_env} æœªæ‰¾åˆ°ï¼Œ"
                    f"ä½¿ç”¨å¤‡ç”¨ç¯å¢ƒå˜é‡ {fallback_env}"
                )
                return api_key
        
        # éƒ½æœªæ‰¾åˆ°
        self.logger.warning(f"âŒ æœªæ‰¾åˆ°ç¯å¢ƒå˜é‡ {primary_env}")
        if "fallback_env" in self.provider_config:
            self.logger.warning(f"   ä¹Ÿæœªæ‰¾åˆ°å¤‡ç”¨ç¯å¢ƒå˜é‡ {self.provider_config['fallback_env']}")
        
        return None
    
    def _init_tokenizer(self):
        """åˆå§‹åŒ–tokenizer"""
        if not TIKTOKEN_AVAILABLE:
            return None
        
        try:
            encoding_name = self.model_config["encoding"]
            tokenizer = tiktoken.get_encoding(encoding_name)
            self.logger.debug(f"ä½¿ç”¨tiktokenç¼–ç å™¨: {encoding_name}")
            return tokenizer
        except Exception as e:
            self.logger.warning(f"tiktokenåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•ä¼°ç®—")
            return None
    
    def _init_client(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        try:
            client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            self.logger.debug(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {self.base_url}")
            return client
        except Exception as e:
            self.logger.error(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _log_initialization(self):
        """è®°å½•åˆå§‹åŒ–ä¿¡æ¯"""
        self.logger.info(f"ğŸ¤– åˆå§‹åŒ–LLMå®¢æˆ·ç«¯")
        self.logger.info(f"   æ¨¡å‹: {self.model_name}")
        self.logger.info(f"   æä¾›å•†: {self.provider}")
        self.logger.info(f"   APIåœ°å€: {self.base_url}")
        self.logger.info(f"   å®é™…æ¨¡å‹: {self.actual_model}")
        self.logger.info(f"ğŸ“ ä¸Šä¸‹æ–‡é•¿åº¦: {self.context_length:,}, å¯ç”¨: {self.max_context_tokens:,}")
        self.logger.info(f"ğŸ“¤ æœ€å¤§è¾“å‡º: {self.max_output_tokens:,}, é»˜è®¤: {self.default_output_tokens:,}")
        self.logger.info(f"ğŸ”§ Tokenè®¡ç®—: {'tiktoken' if self.tokenizer else 'ç®€å•ä¼°ç®—'}")
        self.logger.info(f"ğŸ“ JSONæ ¼å¼æ”¯æŒ: {'æ˜¯' if self.supports_json_format else 'å¦'}")
    
    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®—tokenæ•°é‡
        ä¼˜å…ˆä½¿ç”¨tiktokenï¼Œfallbackåˆ°ç®€åŒ–ä¼°ç®—
        """
        if self.tokenizer:
            try:
                return len(self.tokenizer.encode(text))
            except Exception:
                pass
        
        # ç®€åŒ–çš„tokenä¼°ç®—
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        other_chars = len(text) - chinese_chars - english_chars
        
        estimated_tokens = int(
            chinese_chars * 0.6 +
            english_chars * 0.3 +
            other_chars * 0.4
        )
        
        return max(estimated_tokens, 1)
    
    def truncate_context(self, context_text: str, max_tokens: Optional[int] = None) -> str:
        """
        ç®€åŒ–çš„ä¸Šä¸‹æ–‡æˆªæ–­
        ä¿ç•™æœ€åçš„å†…å®¹ï¼ˆæœ€æ–°çš„å¯¹è¯é€šå¸¸æœ€é‡è¦ï¼‰
        """
        if max_tokens is None:
            max_tokens = self.max_context_tokens
        
        current_tokens = self.count_tokens(context_text)
        
        if current_tokens <= max_tokens:
            return context_text
        
        # æŒ‰è¡Œæˆªæ–­ï¼Œä»åå¾€å‰ä¿ç•™
        lines = context_text.split('\n')
        selected_lines = []
        current_tokens = 0
        
        for line in reversed(lines):
            line_tokens = self.count_tokens(line + '\n')
            if current_tokens + line_tokens <= max_tokens:
                selected_lines.insert(0, line)
                current_tokens += line_tokens
            else:
                break
        
        result = '\n'.join(selected_lines)
        final_tokens = self.count_tokens(result)
        
        if final_tokens != current_tokens:
            self.logger.info(f"ğŸ“ ä¸Šä¸‹æ–‡æˆªæ–­: {self.count_tokens(context_text)} -> {final_tokens} tokens")
        
        return result
    
    def generate_answer(self, 
                       prompt: str, 
                       max_tokens: Optional[int] = None,
                       temperature: float = 0.1,
                       generate_strategy: str = "default",
                       json_format: bool = False,
                       **kwargs) -> str:
        """
        ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            prompt: å®Œæ•´æç¤ºè¯
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            generate_strategy: ç”Ÿæˆç­–ç•¥ ("default", "max")
            json_format: æ˜¯å¦è¦æ±‚JSONæ ¼å¼è¾“å‡º
            **kwargs: å…¶ä»–APIå‚æ•°
            
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        
        # è®¾ç½®è¾“å‡ºtokenæ•°
        if max_tokens is None:
            if generate_strategy == "max":
                max_tokens = self.max_output_tokens
            else:
                max_tokens = self.default_output_tokens
        else:
            max_tokens = min(max_tokens, self.max_output_tokens)
        
        # ç¡®ä¿promptä¸è¶…è¿‡ä¸Šä¸‹æ–‡é™åˆ¶
        prompt_tokens = self.count_tokens(prompt)
        max_prompt_tokens = self.context_length - max_tokens - 100
        
        if prompt_tokens > max_prompt_tokens:
            self.logger.warning(f"âš ï¸ Promptè¿‡é•¿ ({prompt_tokens} > {max_prompt_tokens})ï¼Œæˆªæ–­ä¸­...")
            prompt = self.truncate_context(prompt, max_prompt_tokens)
            prompt_tokens = self.count_tokens(prompt)
            self.logger.info(f"ğŸ“ Promptæˆªæ–­å: {prompt_tokens} tokens")
        
        try:
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "model": self.actual_model,  # ä½¿ç”¨å®é™…æ¨¡å‹å
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": temperature
            }
            
            # æ·»åŠ å…¶ä»–æ”¯æŒçš„å‚æ•°
            supported_params = ['frequency_penalty', 'presence_penalty', 'top_p', 'stop']
            for param in supported_params:
                if param in kwargs:
                    request_params[param] = kwargs[param]
            
            # JSONæ ¼å¼è¾“å‡ºæ”¯æŒï¼ˆç»Ÿä¸€å¤„ç†ï¼‰
            if json_format and self.supports_json_format:
                request_params["response_format"] = {"type": "json_object"}
                # ç¡®ä¿promptä¸­åŒ…å«JSONæŒ‡ä»¤
                if "json" not in prompt.lower():
                    prompt += "\n\nPlease respond in JSON format."
                    request_params["messages"] = [{"role": "user", "content": prompt}]
                self.logger.debug("âœ… å·²å¯ç”¨JSONæ ¼å¼è¾“å‡º")
            elif json_format and not self.supports_json_format:
                self.logger.warning(f"âš ï¸ æ¨¡å‹ {self.model_name} ä¸æ”¯æŒJSONæ ¼å¼ï¼Œå°†å¿½ç•¥json_formatå‚æ•°")
            
            self.logger.debug(f"ğŸš€ å‘é€è¯·æ±‚: {prompt_tokens} tokens -> max {max_tokens} tokens")
            self.logger.debug(f"ğŸ“¡ API: {self.base_url}, æ¨¡å‹: {self.actual_model}")
            
            response = self.client.chat.completions.create(**request_params)
            
            answer = response.choices[0].message.content.strip()
            
            # è®°å½•tokenä½¿ç”¨
            if hasattr(response, 'usage') and response.usage:
                usage = response.usage
                self.logger.debug(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥={usage.prompt_tokens}, "
                                f"è¾“å‡º={usage.completion_tokens}, "
                                f"æ€»è®¡={usage.total_tokens}")
            else:
                estimated_output = self.count_tokens(answer)
                self.logger.debug(f"ğŸ“Š Tokenä¼°ç®—: è¾“å…¥â‰ˆ{prompt_tokens}, è¾“å‡ºâ‰ˆ{estimated_output}")
            
            return answer
            
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            self.logger.debug(f"Provider: {self.provider}, Model: {self.model_name}, Base URL: {self.base_url}")
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def batch_generate(self, 
                      prompts: List[str], 
                      max_tokens: Optional[int] = None,
                      temperature: float = 0.1,
                      **kwargs) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆç­”æ¡ˆ"""
        results = []
        
        self.logger.info(f"ğŸ”„ æ‰¹é‡ç”Ÿæˆå¼€å§‹: {len(prompts)} ä¸ªè¯·æ±‚")
        
        for i, prompt in enumerate(prompts, 1):
            self.logger.debug(f"å¤„ç† {i}/{len(prompts)}")
            answer = self.generate_answer(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                **kwargs
            )
            results.append(answer)
        
        self.logger.info(f"âœ… æ‰¹é‡ç”Ÿæˆå®Œæˆ")
        return results
    
    def get_context_info(self) -> Dict[str, Any]:
        """è·å–LLMä¸Šä¸‹æ–‡é…ç½®ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "actual_model": self.actual_model,
            "provider": self.provider,
            "base_url": self.base_url,
            "context_length": self.context_length,
            "max_output_tokens": self.max_output_tokens,
            "default_output_tokens": self.default_output_tokens,
            "max_context_tokens": self.max_context_tokens,
            "tokenizer_available": self.tokenizer is not None,
            "encoding": self.model_config.get("encoding", "unknown"),
            "supports_json_format": self.supports_json_format
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®ä¿¡æ¯ï¼ˆåˆ«åæ–¹æ³•ï¼‰"""
        return self.get_context_info()
    
    def analyze_text(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬çš„tokenä¿¡æ¯"""
        total_tokens = self.count_tokens(text)
        
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        
        return {
            "total_tokens": total_tokens,
            "character_count": len(text),
            "chinese_chars": chinese_chars,
            "english_chars": english_chars,
            "tokens_per_char": total_tokens / len(text) if text else 0,
            "fits_in_context": total_tokens <= self.max_context_tokens,
            "usage_ratio": total_tokens / self.context_length,
            "can_process": total_tokens <= (self.context_length - self.default_output_tokens)
        }
    
    @classmethod
    def list_available_models(cls) -> Dict[str, List[str]]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ï¼ŒæŒ‰æä¾›å•†åˆ†ç»„"""
        models_by_provider = {}
        
        for model_name, config in cls.MODEL_CONFIGS.items():
            provider = config["provider"]
            if provider not in models_by_provider:
                models_by_provider[provider] = []
            models_by_provider[provider].append(model_name)
        
        return models_by_provider
    
    @classmethod
    def get_provider_info(cls, provider: str) -> Optional[Dict[str, Any]]:
        """è·å–APIæä¾›å•†ä¿¡æ¯"""
        return cls.API_PROVIDERS.get(provider)
    
    @classmethod
    def list_required_env_vars(cls) -> Dict[str, List[str]]:
        """åˆ—å‡ºæ‰€æœ‰æä¾›å•†éœ€è¦çš„ç¯å¢ƒå˜é‡"""
        env_vars = {}
        
        for provider, config in cls.API_PROVIDERS.items():
            vars_list = [config["api_key_env"]]
            if "fallback_env" in config:
                vars_list.append(f"{config['fallback_env']} (å¤‡ç”¨)")
            env_vars[provider] = vars_list
        
        return env_vars


# ä¾¿æ·å‡½æ•°
def create_llm_client(model: str = "deepseek-reasoner", 
                     api_key: Optional[str] = None,
                     base_url: Optional[str] = None) -> LLMClient:
    """åˆ›å»ºLLMå®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°"""
    return LLMClient(model_name=model, api_key=api_key, base_url=base_url)


def create_deepseek_client(model: str = "deepseek-chat", 
                          api_key: Optional[str] = None) -> LLMClient:
    """åˆ›å»ºDeepSeekå®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰"""
    return LLMClient(model_name=model, api_key=api_key)


def estimate_tokens(text: str) -> int:
    """å¿«é€Ÿä¼°ç®—tokenæ•°çš„ç‹¬ç«‹å‡½æ•°"""
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    other_chars = len(text) - chinese_chars - english_chars
    
    return int(chinese_chars * 0.6 + english_chars * 0.3 + other_chars * 0.4)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import time
    import sys
    
    print("=" * 80)
    print("LLMå®¢æˆ·ç«¯æµ‹è¯•å·¥å…·")
    print("=" * 80)
    
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
    print("\nğŸ“‹ å¯ç”¨æ¨¡å‹:")
    models = LLMClient.list_available_models()
    all_models = []
    model_index = 1
    for provider, model_list in models.items():
        print(f"\n{provider}:")
        for model in model_list:
            print(f"  [{model_index}] {model}")
            all_models.append(model)
            model_index += 1
    
    # é€‰æ‹©æ¨¡å‹
    print("\n" + "=" * 80)
    model_choice = input(f"è¯·é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹ (1-{len(all_models)}, ç›´æ¥å›è½¦é»˜è®¤ä½¿ç”¨ deepseek-v3:671b): ").strip()
    
    if model_choice == "":
        selected_model = "deepseek-v3:671b"
        print(f"ä½¿ç”¨é»˜è®¤æ¨¡å‹: {selected_model}")
    elif model_choice.isdigit() and 1 <= int(model_choice) <= len(all_models):
        selected_model = all_models[int(model_choice) - 1]
        print(f"å·²é€‰æ‹©æ¨¡å‹: {selected_model}")
    else:
        print(f"âŒ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: deepseek-v3:671b")
        selected_model = "deepseek-v3:671b"
    
    # è¾“å…¥æµ‹è¯•é—®é¢˜
    print("\n" + "=" * 80)
    test_prompt = input("è¯·è¾“å…¥æµ‹è¯•é—®é¢˜ (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤é—®é¢˜): ").strip()
    if test_prompt == "":
        test_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½ã€‚"
        print(f"ä½¿ç”¨é»˜è®¤é—®é¢˜: {test_prompt}")
    
    # å¼€å§‹æµ‹è¯•
    print("\n" + "=" * 80)
    print(f"æµ‹è¯•æ¨¡å‹: {selected_model}")
    print("=" * 80)
    
    try:
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        print("\nâ³ æ­£åœ¨åˆå§‹åŒ–å®¢æˆ·ç«¯...")
        init_start = time.time()
        client = LLMClient(selected_model)
        init_time = time.time() - init_start
        print(f"âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {init_time:.2f}ç§’)")
        
        # åˆ†ææµ‹è¯•æ–‡æœ¬
        print("\nğŸ“Š åˆ†ææµ‹è¯•æ–‡æœ¬...")
        test_text = "Hello world! ä½ å¥½ä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚"
        analysis = client.analyze_text(test_text)
        print(f"   æ–‡æœ¬: {test_text}")
        print(f"   Tokenæ•°: {analysis['total_tokens']}")
        print(f"   å­—ç¬¦æ•°: {analysis['character_count']}")
        print(f"   ä¸­æ–‡å­—ç¬¦: {analysis['chinese_chars']}")
        print(f"   è‹±æ–‡å­—ç¬¦: {analysis['english_chars']}")
        
        # ç”Ÿæˆç­”æ¡ˆ
        print(f"\nğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")
        print(f"   é—®é¢˜: {test_prompt}")
        generate_start = time.time()
        answer = client.generate_answer(test_prompt, max_tokens=1000)
        generate_time = time.time() - generate_start
        
        # æ‰“å°ç»“æœ
        print("\n" + "=" * 80)
        print("ğŸ“ ç”Ÿæˆç»“æœ:")
        print("=" * 80)
        print(answer)
        print("=" * 80)
        
        # æ‰“å°æ€§èƒ½ä¿¡æ¯
        print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
        print(f"   åˆå§‹åŒ–è€—æ—¶: {init_time:.2f}ç§’")
        print(f"   ç”Ÿæˆè€—æ—¶: {generate_time:.2f}ç§’")
        print(f"   æ€»è€—æ—¶: {init_time + generate_time:.2f}ç§’")
        
        # åˆ†æå›ç­”
        answer_analysis = client.analyze_text(answer)
        print(f"\nğŸ“Š å›ç­”åˆ†æ:")
        print(f"   å­—ç¬¦æ•°: {answer_analysis['character_count']}")
        print(f"   Tokenæ•°: {answer_analysis['total_tokens']}")
        print(f"   ä¸­æ–‡å­—ç¬¦: {answer_analysis['chinese_chars']}")
        print(f"   è‹±æ–‡å­—ç¬¦: {answer_analysis['english_chars']}")
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        info = client.get_model_info()
        print(f"\nğŸ”§ æ¨¡å‹ä¿¡æ¯:")
        print(f"   é…ç½®æ¨¡å‹: {info['model_name']}")
        print(f"   å®é™…æ¨¡å‹: {info['actual_model']}")
        print(f"   æä¾›å•†: {info['provider']}")
        print(f"   APIåœ°å€: {info['base_url']}")
        print(f"   ä¸Šä¸‹æ–‡é•¿åº¦: {info['context_length']:,}")
        print(f"   æœ€å¤§è¾“å‡º: {info['max_output_tokens']:,}")
        print(f"   é»˜è®¤è¾“å‡º: {info['default_output_tokens']:,}")
        print(f"   æ”¯æŒJSON: {info['supports_json_format']}")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
    
    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 80)