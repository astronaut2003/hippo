"""
LongMemEval Benchmark æµ‹è¯•è„šæœ¬

ä½¿ç”¨ mem0 è®°å¿†ç³»ç»Ÿå’Œ LLM è¿›è¡Œé—®ç­”è¯„ä¼°
"""

import json
import os
import sys
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import logging
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from task_eval.load_dataset_unprocessed import LongMemEvalLoader, load_dataset
from task_eval.llm_client import LLMClient
from task_eval.evaluation import calculate_comprehensive_scores

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LongMemEvalBenchmark:
    """LongMemEval åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(
        self,
        dataset_path: str,
        llm_model: str = "gpt-4o-mini-closeai",
        user_id_base: str = "benchmark",
        infer: bool = False,
        output_dir: str = "benchmark_results"
    ):
        """
        åˆå§‹åŒ– Benchmark
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„
            llm_model: LLM æ¨¡å‹åç§°
            user_id_base: user_id åŸºç¡€åç§°
            infer: æ˜¯å¦å¯ç”¨ mem0 çš„æ¨ç†åŠŸèƒ½
            output_dir: è¾“å‡ºç›®å½•
        """
        self.dataset_path = dataset_path
        self.llm_model = llm_model
        self.user_id_base = user_id_base
        self.infer = infer
        self.output_dir = output_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–åŠ è½½å™¨å’Œ LLM å®¢æˆ·ç«¯
        logger.info("åˆå§‹åŒ– LongMemEval åŠ è½½å™¨...")
        self.loader = LongMemEvalLoader()
        
        logger.info(f"åˆå§‹åŒ– LLM å®¢æˆ·ç«¯: {llm_model}")
        self.llm_client = LLMClient(model_name=llm_model)
        
        logger.info("Benchmark åˆå§‹åŒ–å®Œæˆ")
    
    def format_memories_for_prompt(self, memories: List[Dict[str, Any]]) -> str:
        """
        å°†æ£€ç´¢åˆ°çš„è®°å¿†æ ¼å¼åŒ–ä¸º LLM prompt
        
        Args:
            memories: è®°å¿†åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–åçš„è®°å¿†æ–‡æœ¬
        """
        if not memories:
            return "No relevant memories found."
        
        formatted_parts = []
        for i, mem in enumerate(memories, 1):
            memory_text = mem.get('memory', '')
            score = mem.get('score', 0)
            rerank_score = mem.get('rerank_score', 0)
            
            formatted_parts.append(
                f"Memory {i} (relevance: {score:.3f}, rerank: {rerank_score:.3f}):\n{memory_text}"
            )
        
        return "\n\n".join(formatted_parts)
    
    def create_qa_prompt(
        self,
        question: str,
        memories: List[Dict[str, Any]],
        question_type: str = "unknown"
    ) -> str:
        """
        åˆ›å»ºé—®ç­” prompt
        
        Args:
            question: é—®é¢˜
            memories: æ£€ç´¢åˆ°çš„è®°å¿†
            question_type: é—®é¢˜ç±»å‹
            
        Returns:
            å®Œæ•´çš„ prompt
        """
        memories_text = self.format_memories_for_prompt(memories)
        
        prompt = f"""You are a helpful assistant with access to the user's conversation history and memories.

        Based on the following retrieved memories, please answer the user's question accurately and concisely.

        Question Type: {question_type}

        Retrieved Memories:
        {memories_text}

        User Question: {question}

        Instructions:
        - Answer based ONLY on the information provided in the memories above
        - If the memories don't contain enough information, say "I don't have enough information to answer this question"
        - Be concise and direct
        - For temporal questions, pay attention to dates and chronological order
        - For preference questions, focus on user's stated preferences
        - DO NOT make up information

        Answer:"""
        
        return prompt
    
    def process_single_sample(
        self,
        sample: Dict[str, Any],
        sample_idx: int,
        query_top_k: int = 5
    ) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ ·æœ¬
        
        Args:
            sample: æ ·æœ¬æ•°æ®
            sample_idx: æ ·æœ¬ç´¢å¼•
            query_top_k: æ£€ç´¢è¿”å›çš„è®°å¿†æ•°é‡
            
        Returns:
            å¤„ç†ç»“æœ
        """
        question_id = sample.get('question_id', 'unknown')
        question = sample.get('question', '')
        question_type = sample.get('question_type', 'unknown')
        gold_answer = sample.get('answer', '')
        
        logger.info(f"\n{'='*80}")
        logger.info(f"[æ ·æœ¬ {sample_idx}] {question_id}")
        logger.info(f"é—®é¢˜ç±»å‹: {question_type}")
        logger.info(f"é—®é¢˜: {question}")
        logger.info(f"æ ‡å‡†ç­”æ¡ˆ: {gold_answer}")
        logger.info(f"{'='*80}")
        
        result = {
            'sample_idx': sample_idx,
            'question_id': question_id,
            'question': question,
            'question_type': question_type,
            'gold_answer': gold_answer,
            'user_id': f"{self.user_id_base}_{sample_idx}",
            'status': 'success'
        }
        
        try:
            # 1. åŠ è½½å¯¹è¯å†å²åˆ°è®°å¿†ç³»ç»Ÿ
            logger.info(f"[æ ·æœ¬ {sample_idx}] åŠ è½½å¯¹è¯å†å²...")
            load_result = self.loader.load_sample(
                sample=sample,
                sample_idx=sample_idx,
                user_id_base=self.user_id_base,
                infer=self.infer,
                clean_before_add=True
            )
            
            result['load_result'] = {
                'total_sessions': load_result['add_result']['total_sessions'],
                'added_sessions': load_result['add_result']['added_sessions'],
                'failed_sessions': load_result['add_result']['failed_sessions']
            }
            
            logger.info(
                f"[æ ·æœ¬ {sample_idx}] åŠ è½½å®Œæˆ: "
                f"{load_result['add_result']['added_sessions']}/{load_result['add_result']['total_sessions']} ä¸ªä¼šè¯"
            )
            
            # 2. æ£€ç´¢ç›¸å…³è®°å¿†
            logger.info(f"[æ ·æœ¬ {sample_idx}] æ£€ç´¢ç›¸å…³è®°å¿†...")
            memories = self.loader.search_sample(
                question=question,
                sample_idx=sample_idx,
                user_id_base=self.user_id_base,
                query_top_k=query_top_k
            )
            
            result['retrieved_memories_count'] = len(memories)
            result['retrieved_memories'] = memories
            
            logger.info(f"[æ ·æœ¬ {sample_idx}] æ£€ç´¢åˆ° {len(memories)} æ¡è®°å¿†")
            
            # 3. ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
            logger.info(f"[æ ·æœ¬ {sample_idx}] ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆ...")
            prompt = self.create_qa_prompt(question, memories, question_type)
            
            predicted_answer = self.llm_client.generate_answer(
                prompt=prompt,
                temperature=0.1,
                max_tokens=512
            )
            
            result['predicted_answer'] = predicted_answer
            result['prompt_length'] = len(prompt)
            
            logger.info(f"[æ ·æœ¬ {sample_idx}] é¢„æµ‹ç­”æ¡ˆ: {predicted_answer}")
            
            # 4. è¯„ä¼°ç­”æ¡ˆè´¨é‡
            logger.info(f"[æ ·æœ¬ {sample_idx}] è¯„ä¼°ç­”æ¡ˆè´¨é‡...")
            eval_scores = calculate_comprehensive_scores(
                gold_answer=gold_answer,
                response=predicted_answer,
                question=question,
                question_type=question_type,
                llm_client=self.llm_client,
                metrics=['exact_match', 'f1', 'rouge', 'semantic_similarity']
            )
            
            result['evaluation'] = eval_scores
            
            logger.info(f"[æ ·æœ¬ {sample_idx}] F1åˆ†æ•°: {eval_scores['scores'].get('f1', 0):.3f}")
            
            # 5. æ¸…ç†è®°å¿†
            self.loader.reset_memory(sample_idx=sample_idx, user_id_base=self.user_id_base)
            
        except Exception as e:
            logger.error(f"[æ ·æœ¬ {sample_idx}] å¤„ç†å¤±è´¥: {e}", exc_info=True)
            result['status'] = 'failed'
            result['error'] = str(e)
        
        return result
    
    def run_benchmark(
        self,
        sample_indices: Optional[List[int]] = None,
        query_top_k: int = 5,
        save_results: bool = True
    ) -> Dict[str, Any]:
        """
        è¿è¡ŒåŸºå‡†æµ‹è¯•
        
        Args:
            sample_indices: è¦æµ‹è¯•çš„æ ·æœ¬ç´¢å¼•åˆ—è¡¨ï¼ŒNone è¡¨ç¤ºæµ‹è¯•æ‰€æœ‰æ ·æœ¬
            query_top_k: æ£€ç´¢è¿”å›çš„è®°å¿†æ•°é‡
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            
        Returns:
            æµ‹è¯•ç»“æœ
        """
        logger.info("="*80)
        logger.info("å¼€å§‹ LongMemEval Benchmark æµ‹è¯•")
        logger.info("="*80)
        logger.info(f"æ•°æ®é›†: {self.dataset_path}")
        logger.info(f"LLM æ¨¡å‹: {self.llm_model}")
        logger.info(f"Infer æ¨¡å¼: {self.infer}")
        logger.info(f"æ£€ç´¢ Top-K: {query_top_k}")
        
        # åŠ è½½æ•°æ®é›†
        logger.info("\nåŠ è½½æ•°æ®é›†...")
        samples = load_dataset(self.dataset_path, sample_indices=sample_indices)
        logger.info(f"åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
        
        # å¤„ç†æ¯ä¸ªæ ·æœ¬
        all_results = []
        start_time = datetime.now()
        
        for idx, sample in enumerate(tqdm(samples, desc="å¤„ç†æ ·æœ¬")):
            # ä½¿ç”¨åŸå§‹ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            original_idx = sample.get('sample_index', idx)
            
            result = self.process_single_sample(
                sample=sample,
                sample_idx=original_idx,
                query_top_k=query_top_k
            )
            
            all_results.append(result)
        
        end_time = datetime.now()
        total_time = (end_time - start_time).total_seconds()
        
        # ç»Ÿè®¡ç»“æœ
        logger.info("\n" + "="*80)
        logger.info("æµ‹è¯•å®Œæˆï¼Œç»Ÿè®¡ç»“æœ...")
        logger.info("="*80)
        
        successful = [r for r in all_results if r['status'] == 'success']
        failed = [r for r in all_results if r['status'] == 'failed']
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = self._calculate_average_metrics(successful)
        
        summary = {
            'benchmark_info': {
                'dataset_path': self.dataset_path,
                'llm_model': self.llm_model,
                'user_id_base': self.user_id_base,
                'infer': self.infer,
                'query_top_k': query_top_k,
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'total_time_seconds': total_time
            },
            'statistics': {
                'total_samples': len(samples),
                'successful': len(successful),
                'failed': len(failed),
                'success_rate': len(successful) / len(samples) if samples else 0,
                'avg_time_per_sample': total_time / len(samples) if samples else 0
            },
            'average_metrics': avg_metrics,
            'detailed_results': all_results
        }
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(summary)
        
        # ä¿å­˜ç»“æœ
        if save_results:
            self._save_results(summary)
        
        return summary
    
    def _calculate_average_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        if not results:
            return {}
        
        metrics = {}
        metric_names = ['exact_match', 'f1', 'semantic_similarity']
        
        for metric_name in metric_names:
            values = []
            for r in results:
                if 'evaluation' in r and 'scores' in r['evaluation']:
                    value = r['evaluation']['scores'].get(metric_name)
                    if value is not None:
                        values.append(value)
            
            if values:
                metrics[f'avg_{metric_name}'] = sum(values) / len(values)
        
        # è®¡ç®—å¹³å‡æ£€ç´¢è®°å¿†æ•°
        memory_counts = [r.get('retrieved_memories_count', 0) for r in results]
        if memory_counts:
            metrics['avg_retrieved_memories'] = sum(memory_counts) / len(memory_counts)
        
        return metrics
    
    def _print_summary(self, summary: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        stats = summary['statistics']
        metrics = summary['average_metrics']
        
        print("\n" + "="*80)
        print("ğŸ“Š æµ‹è¯•æ‘˜è¦")
        print("="*80)
        print(f"æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"æˆåŠŸ: {stats['successful']} | å¤±è´¥: {stats['failed']}")
        print(f"æˆåŠŸç‡: {stats['success_rate']:.2%}")
        print(f"æ€»è€—æ—¶: {stats['avg_time_per_sample']*stats['total_samples']:.2f}ç§’")
        print(f"å¹³å‡è€—æ—¶: {stats['avg_time_per_sample']:.2f}ç§’/æ ·æœ¬")
        
        print("\nğŸ“ˆ å¹³å‡æŒ‡æ ‡:")
        for metric_name, value in metrics.items():
            if metric_name.startswith('avg_'):
                display_name = metric_name.replace('avg_', '').replace('_', ' ').title()
                if 'memories' in metric_name:
                    print(f"  {display_name}: {value:.1f}")
                else:
                    print(f"  {display_name}: {value:.4f}")
        
        print("="*80)
    
    def _save_results(self, summary: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = self.llm_model.replace(':', '_').replace('/', '_')
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        output_file = Path(self.output_dir) / f"benchmark_{model_name}_{timestamp}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ä¿å­˜ç®€åŒ–ç‰ˆç»“æœï¼ˆä¸åŒ…å«è¯¦ç»†è®°å¿†ï¼‰
        summary_lite = {
            'benchmark_info': summary['benchmark_info'],
            'statistics': summary['statistics'],
            'average_metrics': summary['average_metrics'],
            'sample_results': [
                {
                    'sample_idx': r['sample_idx'],
                    'question_id': r['question_id'],
                    'question': r['question'],
                    'gold_answer': r['gold_answer'],
                    'predicted_answer': r.get('predicted_answer', ''),
                    'status': r['status'],
                    'evaluation_scores': r.get('evaluation', {}).get('scores', {})
                }
                for r in summary['detailed_results']
            ]
        }
        
        output_file_lite = Path(self.output_dir) / f"benchmark_{model_name}_{timestamp}_lite.json"
        with open(output_file_lite, 'w', encoding='utf-8') as f:
            json.dump(summary_lite, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ ç®€åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_file_lite}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='LongMemEval Benchmark æµ‹è¯•')
    parser.add_argument(
        '--dataset',
        type=str,
        default='benchmark_longmemeval/dataset/LongMemEval/extracted_samples_index_1.json',
        help='æ•°æ®é›†è·¯å¾„'
    )
    parser.add_argument(
        '--model',
        type=str,
        default='gpt-4o-mini-closeai',
        help='LLM æ¨¡å‹åç§°'
    )
    parser.add_argument(
        '--indices',
        type=str,
        default=None,
        help='æ ·æœ¬ç´¢å¼•èŒƒå›´ï¼Œä¾‹å¦‚: "0,1,2" æˆ– "0-10"'
    )
    parser.add_argument(
        '--top-k',
        type=int,
        default=5,
        help='æ£€ç´¢è¿”å›çš„è®°å¿†æ•°é‡'
    )
    parser.add_argument(
        '--infer',
        action='store_true',
        help='æ˜¯å¦å¯ç”¨ mem0 çš„æ¨ç†åŠŸèƒ½'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='benchmark_results',
        help='è¾“å‡ºç›®å½•'
    )
    
    args = parser.parse_args()
    
    # è§£æç´¢å¼•èŒƒå›´
    sample_indices = None
    if args.indices:
        if '-' in args.indices:
            # èŒƒå›´æ ¼å¼: "0-10"
            start, end = map(int, args.indices.split('-'))
            sample_indices = list(range(start, end + 1))
        else:
            # é€—å·åˆ†éš”æ ¼å¼: "0,1,2"
            sample_indices = [int(x.strip()) for x in args.indices.split(',')]
    
    # åˆ›å»º benchmark å®ä¾‹
    benchmark = LongMemEvalBenchmark(
        dataset_path=args.dataset,
        llm_model=args.model,
        user_id_base='benchmark',
        infer=args.infer,
        output_dir=args.output_dir
    )
    
    # è¿è¡Œæµ‹è¯•
    results = benchmark.run_benchmark(
        sample_indices=sample_indices,
        query_top_k=args.top_k,
        save_results=True
    )
    
    return results


if __name__ == "__main__":
    try:
        results = main()
        print("\nâœ… Benchmark æµ‹è¯•æˆåŠŸå®Œæˆï¼")
    except KeyboardInterrupt:
        print("\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()