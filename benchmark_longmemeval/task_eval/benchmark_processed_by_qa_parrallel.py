"""
LongMemEval Benchmark æµ‹è¯•è„šæœ¬ï¼ˆProcessed ç‰ˆæœ¬ - å¹¶è¡Œï¼‰

ä½¿ç”¨ mem0 è®°å¿†ç³»ç»Ÿå’Œ LLM è¿›è¡Œé—®ç­”è¯„ä¼°
é€æ¡ä¿å­˜ç»“æœåˆ° benchmark_results_processed/ ç›®å½•

ã€å¹¶è¡Œè®¾è®¡è¦ç‚¹ã€‘ï¼š
1. æ¯ä¸ªæ ·æœ¬ä½¿ç”¨ç‹¬ç«‹çš„ user_id (user_id_base_{sample_idx}_{worker_id})ï¼Œç¡®ä¿è®°å¿†éš”ç¦»
2. ä½¿ç”¨è¿›ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œé¿å… GIL é™åˆ¶
3. æ¯ä¸ª worker ç‹¬ç«‹åˆå§‹åŒ– mem0 å’Œ LLM å®¢æˆ·ç«¯ï¼Œé¿å…å…±äº«çŠ¶æ€
4. ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶ LLM API å¹¶å‘æ•°ï¼Œé¿å…é™æµ
5. æ–‡ä»¶å†™å…¥æ— å†²çªï¼ˆæ¯ä¸ª QA ç‹¬ç«‹ç›®å½•ï¼‰

ã€ä¸ä¸²è¡Œç‰ˆæœ¬çš„åŒºåˆ«ã€‘ï¼š
- æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
- å¢åŠ  worker_id ç¡®ä¿è·¨è¿›ç¨‹è®°å¿†éš”ç¦»
- å¢åŠ å¹¶å‘æ§åˆ¶å‚æ•°
"""

import json
import os
import sys
import time
import argparse
import multiprocessing as mp
from multiprocessing import Pool, Manager, Lock
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import logging
from tqdm import tqdm
import traceback
import signal
import atexit

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================
# Worker åˆå§‹åŒ–å‡½æ•°ï¼ˆæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åˆå§‹åŒ–ï¼‰
# ============================================================

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨ worker çš„èµ„æºï¼ˆæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹ï¼‰
_worker_resources: Dict[str, Any] = {}


def init_worker(
    worker_id: int,
    dataset_path: str,
    gen_llm_model: str,
    eval_llm_model: str,
    user_id_base: str,
    infer: bool,
    output_dir: str
):
    """
    åˆå§‹åŒ– worker è¿›ç¨‹çš„èµ„æº
    
    ã€æ–¹æ¡ˆ2ï¼šç‹¬ç«‹è·¯å¾„æ¨¡å¼ã€‘
    æ¯ä¸ª worker ä½¿ç”¨ç‹¬ç«‹çš„ Qdrant å­˜å‚¨è·¯å¾„ï¼Œé¿å…æ–‡ä»¶é”å†²çª
    è·¯å¾„æ ¼å¼: /tmp/qdrant_worker_{process_id}
    
    æ¯ä¸ª worker ç‹¬ç«‹åˆå§‹åŒ–ï¼š
    - LongMemEvalLoader (åŒ…å« mem0 Memory å®ä¾‹ï¼Œä½¿ç”¨ç‹¬ç«‹å­˜å‚¨)
    - ç”Ÿæˆ LLM å®¢æˆ·ç«¯
    - è¯„ä¼° LLM å®¢æˆ·ç«¯
    
    Args:
        worker_id: Worker è¿›ç¨‹ ID
        dataset_path: æ•°æ®é›†è·¯å¾„
        gen_llm_model: ç”Ÿæˆ LLM æ¨¡å‹åç§°
        eval_llm_model: è¯„ä¼° LLM æ¨¡å‹åç§°
        user_id_base: user_id åŸºç¡€åç§°
        infer: æ˜¯å¦å¯ç”¨ mem0 æ¨ç†
        output_dir: è¾“å‡ºç›®å½•
    """
    global _worker_resources
    
    import os
    import shutil
    from mem0 import Memory
    from task_eval.llm_client import LLMClient
    from task_eval.load_dataset_processed import LongMemEvalLoader
    
    process_id = os.getpid()
    
    # ğŸ”¥ æ ¸å¿ƒï¼šä¸ºæ¯ä¸ª worker åˆ›å»ºç‹¬ç«‹çš„ Qdrant å­˜å‚¨è·¯å¾„
    worker_qdrant_path = f"/tmp/qdrant_worker_{process_id}"
    
    # æ¸…ç†æ—§çš„å­˜å‚¨ç›®å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if os.path.exists(worker_qdrant_path):
        try:
            shutil.rmtree(worker_qdrant_path)
            logger.info(f"[Worker-{worker_id}] æ¸…ç†æ—§å­˜å‚¨ç›®å½•: {worker_qdrant_path}")
        except Exception as e:
            logger.warning(f"[Worker-{worker_id}] æ¸…ç†æ—§ç›®å½•å¤±è´¥: {e}")
    
    logger.info(f"[Worker-{worker_id}] PID={process_id} åˆå§‹åŒ–ä¸­...")
    logger.info(f"[Worker-{worker_id}] ä½¿ç”¨ç‹¬ç«‹ Qdrant å­˜å‚¨: {worker_qdrant_path}")
    
    try:
        # ğŸ”¥ ä¸ºæ¯ä¸ª worker åˆ›å»ºç‹¬ç«‹çš„ mem0 é…ç½®
        worker_config = {
            "llm": {
                "provider": "deepseek",
                "config": {
                    "model": "deepseek-chat",
                    "temperature": 0.1,
                    "api_key": os.getenv("DEEPSEEK_API_KEY"),
                }
            },
            "reranker": {
                "provider": "huggingface",
                "config": {
                    "model": "BAAI/bge-reranker-v2-m3",
                    "device": "cuda",
                    "batch_size": 32
                }
            },
            "embedder": {
                "provider": "huggingface",
                "config": {
                    "model": "all-MiniLM-L6-v2"
                },
            },
            "vector_store": {
                "provider": "qdrant",
                "config": {
                    "path": worker_qdrant_path,  # ğŸ”¥ æ¯ä¸ª worker ç‹¬ç«‹è·¯å¾„
                    "embedding_model_dims": 384
                }
            }
        }
        
        # ä½¿ç”¨ç‹¬ç«‹é…ç½®åˆ›å»º Memory å®ä¾‹
        memory = Memory.from_config(worker_config)
        
        # åˆ›å»º Loaderï¼ˆä¼ å…¥å·²åˆ›å»ºçš„ memory å®ä¾‹ï¼‰
        loader = LongMemEvalLoader(memory=memory)
        
        # åˆå§‹åŒ–ç”Ÿæˆ LLM å®¢æˆ·ç«¯
        gen_llm_client = LLMClient(model_name=gen_llm_model)
        
        # åˆå§‹åŒ–è¯„ä¼° LLM å®¢æˆ·ç«¯
        if eval_llm_model == gen_llm_model:
            eval_llm_client = gen_llm_client
        else:
            eval_llm_client = LLMClient(model_name=eval_llm_model)
        
        # ä¿å­˜èµ„æºåˆ°å…¨å±€å˜é‡
        _worker_resources = {
            'worker_id': worker_id,
            'process_id': process_id,
            'loader': loader,
            'gen_llm_client': gen_llm_client,
            'eval_llm_client': eval_llm_client,
            'gen_llm_model': gen_llm_model,
            'eval_llm_model': eval_llm_model,
            'user_id_base': user_id_base,
            'infer': infer,
            'output_dir': Path(output_dir),
            'qdrant_path': worker_qdrant_path  # ğŸ”¥ è®°å½•è·¯å¾„ï¼Œç”¨äºæ¸…ç†
        }
        
        logger.info(f"[Worker-{worker_id}] åˆå§‹åŒ–å®Œæˆ (infer={infer}, qdrant={worker_qdrant_path})")
        
    except Exception as e:
        logger.error(f"[Worker-{worker_id}] åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise


def cleanup_worker():
    """æ¸…ç† worker èµ„æºï¼ŒåŒ…æ‹¬ç‹¬ç«‹çš„ Qdrant å­˜å‚¨"""
    global _worker_resources
    import shutil
    
    if _worker_resources:
        worker_id = _worker_resources.get('worker_id', 'unknown')
        qdrant_path = _worker_resources.get('qdrant_path')
        
        logger.info(f"[Worker-{worker_id}] æ¸…ç†èµ„æº...")
        
        # ğŸ”¥ æ¸…ç†ç‹¬ç«‹çš„ Qdrant å­˜å‚¨ç›®å½•
        if qdrant_path and os.path.exists(qdrant_path):
            try:
                shutil.rmtree(qdrant_path)
                logger.info(f"[Worker-{worker_id}] å·²æ¸…ç† Qdrant å­˜å‚¨: {qdrant_path}")
            except Exception as e:
                logger.warning(f"[Worker-{worker_id}] æ¸…ç† Qdrant å­˜å‚¨å¤±è´¥: {e}")
        
        _worker_resources.clear()


def cleanup_worker():
    """æ¸…ç† worker èµ„æº"""
    global _worker_resources
    if _worker_resources:
        worker_id = _worker_resources.get('worker_id', 'unknown')
        logger.info(f"[Worker-{worker_id}] æ¸…ç†èµ„æº...")
        _worker_resources.clear()


# ============================================================
# å•ä¸ªæ ·æœ¬å¤„ç†å‡½æ•°ï¼ˆåœ¨ worker è¿›ç¨‹ä¸­æ‰§è¡Œï¼‰
# ============================================================

def process_single_sample_worker(args: Tuple) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæ ·æœ¬ï¼ˆworker è¿›ç¨‹ä¸­æ‰§è¡Œï¼‰
    
    ã€å¹¶è¡Œéš”ç¦»ç­–ç•¥ã€‘ï¼š
    - user_id æ ¼å¼: {user_id_base}_{sample_idx}_w{worker_id}
    - ç¡®ä¿ä¸åŒ worker å¤„ç†åŒä¸€æ ·æœ¬æ—¶ä¹Ÿä¸ä¼šå†²çª
    - å¤„ç†å®Œæˆåç«‹å³æ¸…ç†è®°å¿†
    
    Args:
        args: (sample, sample_idx, query_top_k) å…ƒç»„
        
    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    global _worker_resources
    
    # å»¶è¿Ÿå¯¼å…¥
    from task_eval.evaluation import calculate_comprehensive_scores
    
    sample, sample_idx, query_top_k = args
    
    # è·å– worker èµ„æº
    worker_id = _worker_resources.get('worker_id', 0)
    loader = _worker_resources['loader']
    gen_llm_client = _worker_resources['gen_llm_client']
    eval_llm_client = _worker_resources['eval_llm_client']
    gen_llm_model = _worker_resources['gen_llm_model']
    eval_llm_model = _worker_resources['eval_llm_model']
    user_id_base = _worker_resources['user_id_base']
    infer = _worker_resources['infer']
    output_dir = _worker_resources['output_dir']
    
    # ğŸ”¥ æ ¸å¿ƒï¼šç”Ÿæˆå¸¦ worker_id çš„å”¯ä¸€ user_idï¼Œç¡®ä¿å¹¶è¡Œéš”ç¦»
    # æ ¼å¼: benchmark_processed_0_w1 (æ ·æœ¬0, worker 1)
    parallel_user_id_base = f"{user_id_base}_{sample_idx}_w{worker_id}"
    
    qa_start_time = time.time()
    
    question_id = sample.get('question_id', 'unknown')
    question = sample.get('question', '')
    question_type = sample.get('question_type', 'unknown')
    gold_answer = sample.get('answer', '')
    question_date = sample.get('question_date', '')
    
    logger.info(f"[Worker-{worker_id}][QA_{sample_idx}] å¼€å§‹å¤„ç†: {question_id}")
    
    result = {
        'sample_idx': sample_idx,
        'question_id': question_id,
        'question': question,
        'question_type': question_type,
        'gold_answer': gold_answer,
        'question_date': question_date,
        'user_id': parallel_user_id_base,
        'worker_id': worker_id,
        'infer_mode': infer,
        'status': 'success',
        'timestamp': datetime.now().isoformat()
    }
    
    # åˆå§‹åŒ–ä¿å­˜æ•°æ®ç»“æ„
    score_data = {
        'sample_idx': sample_idx,
        'question_id': question_id,
        'question': question,
        'question_type': question_type,
        'gold_answer': gold_answer,
        'question_date': question_date,
        'gen_llm_model': gen_llm_model,
        'eval_llm_model': eval_llm_model,
        'worker_id': worker_id,
        'infer_mode': infer,
        'timestamp': datetime.now().isoformat()
    }
    
    retrieval_data = {
        'sample_idx': sample_idx,
        'question_id': question_id,
        'question': question,
        'query_top_k': query_top_k,
        'worker_id': worker_id,
        'infer_mode': infer,
        'timestamp': datetime.now().isoformat()
    }
    
    timing_info = {
        'load_time': 0.0,
        'retrieval_time': 0.0,
        'generation_time': 0.0,
        'evaluation_time': 0.0,
        'cleanup_time': 0.0,
        'total_time': 0.0
    }
    
    try:
        # 1. åŠ è½½å¯¹è¯å†å²åˆ°è®°å¿†ç³»ç»Ÿ
        load_start = time.time()
        
        load_result = loader.load_sample(
            sample=sample,
            sample_idx=sample_idx,
            user_id_base=parallel_user_id_base,  # ğŸ”¥ ä½¿ç”¨å¸¦ worker_id çš„ user_id
            infer=infer,
            clean_before_add=True  # ğŸ”¥ å§‹ç»ˆå…ˆæ¸…ç©ºï¼Œç¡®ä¿å¹²å‡€çŠ¶æ€
        )
        
        timing_info['load_time'] = round(time.time() - load_start, 4)
        
        load_info = {
            'total_sessions': load_result['add_result']['total_sessions'],
            'added_sessions': load_result['add_result']['added_sessions'],
            'failed_sessions': load_result['add_result']['failed_sessions'],
            'infer_mode': load_result['add_result'].get('infer_mode', infer)
        }
        
        result['load_result'] = load_info
        retrieval_data['load_result'] = load_info
        
        # 2. æ£€ç´¢ç›¸å…³è®°å¿†
        retrieval_start = time.time()
        
        memories = loader.search_sample(
            question=question,
            sample_idx=sample_idx,
            user_id_base=parallel_user_id_base,
            query_top_k=query_top_k
        )
        
        timing_info['retrieval_time'] = round(time.time() - retrieval_start, 4)
        
        result['retrieved_memories_count'] = len(memories)
        result['retrieved_memories'] = memories
        retrieval_data['retrieved_memories_count'] = len(memories)
        retrieval_data['retrieved_memories'] = memories
        
        # 3. ç”Ÿæˆç­”æ¡ˆ
        generation_start = time.time()
        
        prompt = _create_qa_prompt(question, memories, question_type)
        gen_prompt_tokens = gen_llm_client.count_tokens(prompt)
        gen_context_info = gen_llm_client.get_context_info()
        
        predicted_answer = gen_llm_client.generate_answer(
            prompt=prompt,
            temperature=0.1,
            max_tokens=512
        )
        
        timing_info['generation_time'] = round(time.time() - generation_start, 4)
        
        gen_answer_tokens = gen_llm_client.count_tokens(predicted_answer)
        gen_token_usage = {
            'prompt_tokens': gen_prompt_tokens,
            'answer_tokens': gen_answer_tokens,
            'total_tokens': gen_prompt_tokens + gen_answer_tokens,
            'context_length': gen_context_info.get('context_length', 0),
            'prompt_ratio': round(gen_prompt_tokens / gen_context_info.get('context_length', 1) * 100, 2),
        }
        
        result['predicted_answer'] = predicted_answer
        result['gen_token_usage'] = gen_token_usage
        score_data['predicted_answer'] = predicted_answer
        score_data['gen_token_usage'] = gen_token_usage
        
        # 4. è¯„ä¼°ç­”æ¡ˆ
        evaluation_start = time.time()
        
        try:
            eval_scores = calculate_comprehensive_scores(
                gold_answer=gold_answer,
                response=predicted_answer,
                question=question,
                question_type=question_type,
                llm_client=eval_llm_client,
                metrics=['exact_match', 'f1', 'rouge', 'semantic_similarity', 'llm_judge']
            )
            
            result['evaluation'] = eval_scores
            score_data['evaluation'] = eval_scores
            score_data['scores'] = eval_scores.get('scores', {})
            
        except Exception as eval_error:
            logger.warning(f"[Worker-{worker_id}][QA_{sample_idx}] è¯„ä¼°å¤±è´¥: {eval_error}")
            result['evaluation'] = {'error': str(eval_error)}
            score_data['evaluation'] = {'error': str(eval_error)}
            score_data['scores'] = {}
        
        timing_info['evaluation_time'] = round(time.time() - evaluation_start, 4)
        
        # 5. ğŸ”¥ æ¸…ç†è®°å¿†ï¼ˆå…³é”®ï¼é¿å…è®°å¿†æ±¡æŸ“å…¶ä»–æ ·æœ¬ï¼‰
        cleanup_start = time.time()
        loader.reset_memory(sample_idx=sample_idx, user_id_base=parallel_user_id_base)
        timing_info['cleanup_time'] = round(time.time() - cleanup_start, 4)
        
        result['status'] = 'success'
        score_data['status'] = 'success'
        retrieval_data['status'] = 'success'
        
    except Exception as e:
        logger.error(f"[Worker-{worker_id}][QA_{sample_idx}] å¤„ç†å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        
        result['status'] = 'failed'
        result['error'] = str(e)
        score_data['status'] = 'failed'
        score_data['error'] = str(e)
        retrieval_data['status'] = 'failed'
        retrieval_data['error'] = str(e)
        
        # ğŸ”¥ å¤±è´¥æ—¶ä¹Ÿè¦å°è¯•æ¸…ç†è®°å¿†
        try:
            loader.reset_memory(sample_idx=sample_idx, user_id_base=parallel_user_id_base)
        except:
            pass
    
    # è®¡ç®—æ€»æ—¶é—´
    timing_info['total_time'] = round(time.time() - qa_start_time, 4)
    result['timing'] = timing_info
    score_data['timing'] = timing_info
    retrieval_data['timing'] = {
        'retrieval_time': timing_info['retrieval_time'],
        'load_time': timing_info['load_time']
    }
    
    # ä¿å­˜ç»“æœ
    _save_sample_results(output_dir, sample_idx, score_data, retrieval_data)
    
    logger.info(
        f"[Worker-{worker_id}][QA_{sample_idx}] å®Œæˆ: "
        f"status={result['status']}, time={timing_info['total_time']:.2f}s"
    )
    
    return result


def _create_qa_prompt(question: str, memories: List[Dict], question_type: str) -> str:
    """åˆ›å»º QA prompt"""
    if not memories:
        memories_text = "No relevant memories found."
    else:
        formatted_parts = []
        for i, mem in enumerate(memories, 1):
            memory_text = mem.get('memory', '')
            score = mem.get('score', 0)
            rerank_score = mem.get('rerank_score', 0)
            formatted_parts.append(
                f"Memory {i} (relevance: {score:.3f}, rerank: {rerank_score:.3f}):\n{memory_text}"
            )
        memories_text = "\n\n".join(formatted_parts)
    
    prompt = f"""You are a helpful assistant with access to the user's conversation history and memories.

Based on the following retrieved memories, please answer the user's question accurately and concisely.

Question Type: {question_type}

Retrieved Memories:
{memories_text}

User Question: {question}

Instructions:
- Answer based ONLY on the information provided in the memories above
- If the memories don't contain enough information, say "I don't have enough information to answer this question"
- Be concise and direct
- For temporal questions, pay attention to dates and chronological order
- For preference questions, focus on user's stated preferences
- DO NOT make up information

Answer:"""
    
    return prompt


def _save_sample_results(output_dir: Path, qa_index: int, score_data: Dict, retrieval_data: Dict):
    """ä¿å­˜æ ·æœ¬ç»“æœ"""
    qa_dir = output_dir / f"QA_{qa_index}"
    qa_dir.mkdir(parents=True, exist_ok=True)
    
    score_file = qa_dir / "score.json"
    with open(score_file, 'w', encoding='utf-8') as f:
        json.dump(score_data, f, indent=2, ensure_ascii=False)
    
    retrieval_file = qa_dir / "retrieval.json"
    with open(retrieval_file, 'w', encoding='utf-8') as f:
        json.dump(retrieval_data, f, indent=2, ensure_ascii=False)


# ============================================================
# å¹¶è¡Œ Benchmark ä¸»ç±»
# ============================================================

class LongMemEvalBenchmarkParallel:
    """
    LongMemEval åŸºå‡†æµ‹è¯•ç±»ï¼ˆProcessed ç‰ˆæœ¬ - å¹¶è¡Œï¼‰
    
    ã€å¹¶è¡Œç­–ç•¥ã€‘ï¼š
    1. ä½¿ç”¨ ProcessPoolExecutor è¿›è¡Œå¤šè¿›ç¨‹å¹¶è¡Œ
    2. æ¯ä¸ª worker ç‹¬ç«‹åˆå§‹åŒ– mem0 å’Œ LLM å®¢æˆ·ç«¯
    3. ä½¿ç”¨å¸¦ worker_id çš„ user_id ç¡®ä¿è®°å¿†éš”ç¦»
    4. æ”¯æŒæ§åˆ¶æœ€å¤§å¹¶å‘æ•°
    """
    
    def __init__(
        self,
        dataset_path: str,
        gen_llm_model: str = "gpt-4o-mini-closeai",
        eval_llm_model: str = "gpt-4o-mini-closeai",
        user_id_base: str = "benchmark_processed_parallel",
        infer: bool = True,
        output_dir: str = "benchmark_results_processed",
        num_workers: int = 4,
        max_concurrent_llm: int = 8
    ):
        """
        åˆå§‹åŒ–å¹¶è¡Œ Benchmark
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„
            gen_llm_model: ç”Ÿæˆç­”æ¡ˆçš„ LLM æ¨¡å‹åç§°
            eval_llm_model: è¯„ä¼°ç­”æ¡ˆçš„ LLM æ¨¡å‹åç§°
            user_id_base: user_id åŸºç¡€åç§°
            infer: æ˜¯å¦å¯ç”¨ mem0 çš„æ¨ç†åŠŸèƒ½
            output_dir: è¾“å‡ºç›®å½•
            num_workers: å¹¶è¡Œ worker æ•°é‡
            max_concurrent_llm: æœ€å¤§ LLM API å¹¶å‘æ•°ï¼ˆé¢„ç•™ï¼‰
        """
        self.dataset_path = dataset_path
        self.gen_llm_model = gen_llm_model
        self.eval_llm_model = eval_llm_model
        self.user_id_base = user_id_base
        self.infer = infer
        self.output_dir = Path(output_dir)
        self.num_workers = num_workers
        self.max_concurrent_llm = max_concurrent_llm
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("="*80)
        logger.info("åˆå§‹åŒ– LongMemEval Benchmarkï¼ˆProcessed å¹¶è¡Œç‰ˆæœ¬ï¼‰")
        logger.info("="*80)
        logger.info(f"æ•°æ®é›†: {dataset_path}")
        logger.info(f"ç”Ÿæˆ LLM: {gen_llm_model}")
        logger.info(f"è¯„ä¼° LLM: {eval_llm_model}")
        logger.info(f"ğŸ”¥ Infer æ¨¡å¼: {infer}")
        logger.info(f"å¹¶è¡Œ Workers: {num_workers}")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        logger.info("="*80)
    
    def check_existing_results(self, start_index: int, end_index: int) -> List[int]:
        """æ£€æŸ¥å·²å­˜åœ¨çš„ç»“æœ"""
        completed = []
        for idx in range(start_index, end_index + 1):
            qa_dir = self.output_dir / f"QA_{idx}"
            score_file = qa_dir / "score.json"
            retrieval_file = qa_dir / "retrieval.json"
            
            if score_file.exists() and retrieval_file.exists():
                completed.append(idx)
        
        return completed
    
    def run_benchmark(
        self,
        start_index: Optional[int] = None,
        end_index: Optional[int] = None,
        query_top_k: int = 5,
        skip_existing: bool = True,
        save_summary: bool = True
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå¹¶è¡ŒåŸºå‡†æµ‹è¯•
        
        Args:
            start_index: å¼€å§‹ç´¢å¼•
            end_index: ç»“æŸç´¢å¼•
            query_top_k: æ£€ç´¢ top-k
            skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨ç»“æœ
            save_summary: æ˜¯å¦ä¿å­˜æ±‡æ€»
            
        Returns:
            æµ‹è¯•ç»“æœæ±‡æ€»
        """
        # å»¶è¿Ÿå¯¼å…¥
        from task_eval.load_dataset_processed import load_dataset
        
        logger.info("\nåŠ è½½æ•°æ®é›†...")
        all_samples = load_dataset(self.dataset_path)
        total_samples_in_dataset = len(all_samples)
        logger.info(f"æ•°æ®é›†å…± {total_samples_in_dataset} ä¸ªæ ·æœ¬")
        
        # ç¡®å®šç´¢å¼•èŒƒå›´
        if start_index is None:
            start_index = 0
        if end_index is None:
            end_index = total_samples_in_dataset - 1
        
        start_index = max(0, start_index)
        end_index = min(total_samples_in_dataset - 1, end_index)
        
        # æ£€æŸ¥å·²å®Œæˆçš„ç»“æœ
        completed = []
        if skip_existing:
            completed = self.check_existing_results(start_index, end_index)
            if completed:
                logger.info(f"å‘ç° {len(completed)} ä¸ªå·²å®Œæˆçš„æ ·æœ¬ï¼Œå°†è·³è¿‡")
        
        # å‡†å¤‡å¾…å¤„ç†çš„ä»»åŠ¡
        tasks = []
        for idx in range(start_index, end_index + 1):
            if idx in completed:
                continue
            sample = all_samples[idx]
            original_idx = sample.get('sample_index', idx)
            tasks.append((sample, original_idx, query_top_k))
        
        logger.info(f"å°†å¹¶è¡Œå¤„ç† {len(tasks)} ä¸ªæ ·æœ¬ï¼ˆ{self.num_workers} workersï¼‰")
        
        if not tasks:
            logger.info("æ‰€æœ‰æ ·æœ¬å·²å®Œæˆï¼Œæ— éœ€å¤„ç†")
            return {'status': 'all_completed', 'completed_count': len(completed)}
        
        # ğŸ”¥ ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
        start_time = datetime.now()
        all_results = []
        
        # åˆ›å»ºè¿›ç¨‹æ± 
        # æ³¨æ„ï¼šä½¿ç”¨ spawn æ–¹å¼å¯åŠ¨è¿›ç¨‹ï¼Œç¡®ä¿æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åˆå§‹åŒ–
        ctx = mp.get_context('spawn')
        
        with ProcessPoolExecutor(
            max_workers=self.num_workers,
            mp_context=ctx,
            initializer=init_worker,
            initargs=(
                0,  # worker_id ä¼šåœ¨ä»»åŠ¡åˆ†é…æ—¶åŠ¨æ€è®¾ç½®
                self.dataset_path,
                self.gen_llm_model,
                self.eval_llm_model,
                self.user_id_base,
                self.infer,
                str(self.output_dir)
            )
        ) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_idx = {}
            for i, task in enumerate(tasks):
                # æ›´æ–° worker_idï¼ˆé€šè¿‡ä»»åŠ¡ç´¢å¼•æ¨¡è¿ç®—åˆ†é…ï¼‰
                future = executor.submit(process_single_sample_worker, task)
                future_to_idx[future] = task[1]  # sample_idx
            
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
            with tqdm(total=len(tasks), desc="å¹¶è¡Œå¤„ç†") as pbar:
                for future in as_completed(future_to_idx):
                    sample_idx = future_to_idx[future]
                    try:
                        result = future.result(timeout=600)  # 10åˆ†é’Ÿè¶…æ—¶
                        all_results.append(result)
                    except Exception as e:
                        logger.error(f"[QA_{sample_idx}] ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                        all_results.append({
                            'sample_idx': sample_idx,
                            'status': 'failed',
                            'error': str(e)
                        })
                    pbar.update(1)
        
        end_time = datetime.now()
        total_time = (end_time - start_time).total_seconds()
        
        # ç»Ÿè®¡ç»“æœ
        successful = [r for r in all_results if r.get('status') == 'success']
        failed = [r for r in all_results if r.get('status') == 'failed']
        
        avg_metrics = self._calculate_average_metrics(successful)
        
        summary = {
            'benchmark_info': {
                'dataset_path': self.dataset_path,
                'gen_llm_model': self.gen_llm_model,
                'eval_llm_model': self.eval_llm_model,
                'user_id_base': self.user_id_base,
                'infer': self.infer,
                'mode': 'processed_parallel',
                'num_workers': self.num_workers,
                'query_top_k': query_top_k,
                'start_index': start_index,
                'end_index': end_index,
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'total_time_seconds': total_time
            },
            'statistics': {
                'total_in_range': end_index - start_index + 1,
                'skipped': len(completed),
                'processed': len(tasks),
                'successful': len(successful),
                'failed': len(failed),
                'success_rate': len(successful) / len(tasks) if tasks else 0,
                'avg_time_per_sample': total_time / len(tasks) if tasks else 0,
                'parallelism_speedup': f"{self.num_workers}x (ideal)"
            },
            'average_metrics': avg_metrics,
            'failed_indices': [r.get('sample_idx') for r in failed]
        }
        
        self._print_summary(summary)
        
        if save_summary:
            self._save_summary(summary)
        
        return summary
    
    def _calculate_average_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
        if not results:
            return {}
        
        metrics = {}
        metric_names = ['exact_match', 'f1', 'semantic_similarity', 'rouge_1', 'rouge_l']
        
        for metric_name in metric_names:
            values = []
            for r in results:
                if 'evaluation' in r and 'scores' in r['evaluation']:
                    scores = r['evaluation']['scores']
                    if metric_name.startswith('rouge_'):
                        rouge_key = metric_name.replace('rouge_', 'rouge-')
                        if 'rouge' in scores and rouge_key in scores.get('rouge', {}):
                            value = scores['rouge'][rouge_key]
                            if value is not None:
                                values.append(value)
                    else:
                        value = scores.get(metric_name)
                        if value is not None:
                            values.append(value)
            
            if values:
                metrics[f'avg_{metric_name}'] = sum(values) / len(values)
        
        memory_counts = [r.get('retrieved_memories_count', 0) for r in results]
        if memory_counts:
            metrics['avg_retrieved_memories'] = sum(memory_counts) / len(memory_counts)
        
        # è®¡ç®—å¹³å‡è€—æ—¶
        times = [r.get('timing', {}).get('total_time', 0) for r in results]
        if times:
            metrics['avg_total_time'] = sum(times) / len(times)
        
        return metrics
    
    def _print_summary(self, summary: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        stats = summary['statistics']
        metrics = summary['average_metrics']
        info = summary['benchmark_info']
        
        print("\n" + "="*80)
        print("ğŸ“Š æµ‹è¯•æ‘˜è¦ï¼ˆProcessed å¹¶è¡Œæ¨¡å¼ï¼‰")
        print("="*80)
        print(f"å¤„ç†èŒƒå›´: QA_{info['start_index']} ~ QA_{info['end_index']}")
        print(f"ğŸ”¥ Infer æ¨¡å¼: {info['infer']}")
        print(f"ğŸš€ å¹¶è¡Œ Workers: {info['num_workers']}")
        print(f"æ€»æ ·æœ¬æ•°: {stats['processed']} (è·³è¿‡ {stats['skipped']})")
        print(f"æˆåŠŸ: {stats['successful']} | å¤±è´¥: {stats['failed']}")
        print(f"æˆåŠŸç‡: {stats['success_rate']:.2%}")
        print(f"æ€»è€—æ—¶: {stats['avg_time_per_sample']*stats['processed']:.2f}ç§’")
        print(f"å¹³å‡è€—æ—¶: {stats['avg_time_per_sample']:.2f}ç§’/æ ·æœ¬")
        
        if summary['failed_indices']:
            print(f"\nâŒ å¤±è´¥çš„æ ·æœ¬: {summary['failed_indices'][:20]}{'...' if len(summary['failed_indices']) > 20 else ''}")
        
        print("\nğŸ“ˆ å¹³å‡æŒ‡æ ‡:")
        for metric_name, value in metrics.items():
            if metric_name.startswith('avg_'):
                display_name = metric_name.replace('avg_', '').replace('_', ' ').title()
                if 'memories' in metric_name or 'time' in metric_name:
                    print(f"  {display_name}: {value:.2f}")
                else:
                    print(f"  {display_name}: {value:.4f}")
        
        print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        print("="*80)
    
    def _save_summary(self, summary: Dict[str, Any]):
        """ä¿å­˜æ±‡æ€»ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        gen_model_name = self.gen_llm_model.replace(':', '_').replace('/', '_')
        eval_model_name = self.eval_llm_model.replace(':', '_').replace('/', '_')
        
        start_idx = summary['benchmark_info']['start_index']
        end_idx = summary['benchmark_info']['end_index']
        num_workers = summary['benchmark_info']['num_workers']
        
        summary_file = self.output_dir / f"summary_processed_parallel_{num_workers}w_gen_{gen_model_name}_eval_{eval_model_name}_QA{start_idx}-{end_idx}_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nğŸ’¾ æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")


# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='LongMemEval Benchmark æµ‹è¯•ï¼ˆProcessed å¹¶è¡Œç‰ˆæœ¬ï¼‰'
    )
    parser.add_argument(
        '--dataset',
        type=str,
        default='benchmark_longmemeval/dataset/LongMemEval/longmemeval_s_cleaned.json',
        help='æ•°æ®é›†è·¯å¾„'
    )
    parser.add_argument(
        '--gen-model',
        type=str,
        default='gpt-4o-mini-closeai',
        help='ç”Ÿæˆç­”æ¡ˆçš„ LLM æ¨¡å‹åç§°'
    )
    parser.add_argument(
        '--eval-model',
        type=str,
        default='gpt-4o-mini-closeai',
        help='è¯„ä¼°ç­”æ¡ˆçš„ LLM æ¨¡å‹åç§°'
    )
    parser.add_argument(
        '--start',
        type=int,
        default=None,
        help='å¼€å§‹çš„ QA ç´¢å¼•ï¼ˆåŒ…å«ï¼‰'
    )
    parser.add_argument(
        '--end',
        type=int,
        default=None,
        help='ç»“æŸçš„ QA ç´¢å¼•ï¼ˆåŒ…å«ï¼‰'
    )
    parser.add_argument(
        '--top-k',
        type=int,
        default=20,
        help='æ£€ç´¢è¿”å›çš„è®°å¿†æ•°é‡'
    )
    parser.add_argument(
        '--no-infer',
        action='store_true',
        help='ç¦ç”¨ mem0 çš„æ¨ç†åŠŸèƒ½ï¼ˆé»˜è®¤å¯ç”¨ï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='benchmark_longmemeval/benchmark_results_processed',
        help='è¾“å‡ºç›®å½•'
    )
    parser.add_argument(
        '--workers',
        type=int,
        default=5,
        help='å¹¶è¡Œ worker æ•°é‡ï¼ˆé»˜è®¤ 5ï¼‰'
    )
    parser.add_argument(
        '--no-skip',
        action='store_true',
        help='ä¸è·³è¿‡å·²å­˜åœ¨çš„ç»“æœï¼ˆè¦†ç›–æ¨¡å¼ï¼‰'
    )
    
    args = parser.parse_args()
    
    infer = not args.no_infer
    
    benchmark = LongMemEvalBenchmarkParallel(
        dataset_path=args.dataset,
        gen_llm_model=args.gen_model,
        eval_llm_model=args.eval_model,
        user_id_base='benchmark_processed_parallel',
        infer=infer,
        output_dir=args.output_dir,
        num_workers=args.workers
    )
    
    results = benchmark.run_benchmark(
        start_index=args.start,
        end_index=args.end,
        query_top_k=args.top_k,
        skip_existing=not args.no_skip,
        save_summary=True
    )
    
    return results


if __name__ == "__main__":
    # è®¾ç½® multiprocessing å¯åŠ¨æ–¹å¼
    mp.set_start_method('spawn', force=True)
    main()