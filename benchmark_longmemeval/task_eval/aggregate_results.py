"""
å®éªŒæ•°æ®æ±‡æ€»è„šæœ¬

è¯»å–æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰ QA_X å­ç›®å½•ä¸­çš„ score.json å’Œ retrieval.jsonï¼Œ
æ±‡æ€»è¯„åˆ†ã€token å ç”¨ã€æ—¶é—´ç­‰æŒ‡æ ‡ï¼Œè¾“å‡ºä¸º Excel è¡¨æ ¼ã€‚
"""

import os
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

import pandas as pd
from openpyxl.styles import Font, Alignment, Border, Side, PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.utils import get_column_letter


def load_qa_results(qa_dir: Path) -> Dict[str, Any]:
    """
    åŠ è½½å•ä¸ª QA ç›®å½•çš„ç»“æœ
    
    Args:
        qa_dir: QA ç›®å½•è·¯å¾„
        
    Returns:
        åˆå¹¶åçš„ç»“æœå­—å…¸
    """
    result = {
        'qa_index': None,
        'status': 'missing'
    }
    
    # ä»ç›®å½•åæå–ç´¢å¼•
    dir_name = qa_dir.name
    if dir_name.startswith('QA_'):
        try:
            result['qa_index'] = int(dir_name[3:])
        except ValueError:
            result['qa_index'] = dir_name
    
    score_file = qa_dir / 'score.json'
    retrieval_file = qa_dir / 'retrieval.json'
    
    # åŠ è½½ score.json
    if score_file.exists():
        try:
            with open(score_file, 'r', encoding='utf-8') as f:
                score_data = json.load(f)
                result['score_data'] = score_data
                
                # ================= ä¿®æ”¹å¼€å§‹ =================
                # ä¼˜å…ˆæ£€æŸ¥ evaluation_successã€‚
                # å³ä½¿ status='failed' (ä¾‹å¦‚å› æ•°æ®åº“åªè¯»å¯¼è‡´)ï¼Œåªè¦è¯„ä¼°æˆåŠŸå®Œæˆï¼Œå°±è§†ä¸º successã€‚
                eval_info = score_data.get('evaluation', {})
                if isinstance(eval_info, dict) and eval_info.get('evaluation_success') is True:
                    result['status'] = 'success'
                else:
                    # å¦åˆ™ä½¿ç”¨åŸå§‹çŠ¶æ€
                    result['status'] = score_data.get('status', 'unknown')
                # ================= ä¿®æ”¹ç»“æŸ =================
                
        except Exception as e:
            result['score_error'] = str(e)
            result['status'] = 'error'
    
    # åŠ è½½ retrieval.json
    if retrieval_file.exists():
        try:
            with open(retrieval_file, 'r', encoding='utf-8') as f:
                retrieval_data = json.load(f)
                result['retrieval_data'] = retrieval_data
        except Exception as e:
            result['retrieval_error'] = str(e)
    
    return result


def extract_metrics(result: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»ç»“æœä¸­æå–å…³é”®æŒ‡æ ‡
    
    Args:
        result: load_qa_results è¿”å›çš„ç»“æœ
        
    Returns:
        æ‰å¹³åŒ–çš„æŒ‡æ ‡å­—å…¸
    """
    metrics = {
        # åŸºæœ¬ä¿¡æ¯
        'qa_index': result.get('qa_index'),
        'status': result.get('status', 'unknown'),
        # æ·»åŠ é”™è¯¯ä¿¡æ¯å­—æ®µï¼Œæ–¹ä¾¿æ’æŸ¥
        'score_error': result.get('score_error', ''),
        'retrieval_error': result.get('retrieval_error', '')
    }
    
    score_data = result.get('score_data', {})
    retrieval_data = result.get('retrieval_data', {})
    
    # é—®é¢˜ä¿¡æ¯
    metrics['question_id'] = score_data.get('question_id', '')
    metrics['question'] = score_data.get('question', '')
    metrics['question_type'] = score_data.get('question_type', '')
    metrics['gold_answer'] = score_data.get('gold_answer', '')
    metrics['predicted_answer'] = score_data.get('predicted_answer', '')
    metrics['question_date'] = score_data.get('question_date', '')
    
    # æ¨¡å‹ä¿¡æ¯
    metrics['gen_llm_model'] = score_data.get('gen_llm_model', '')
    metrics['eval_llm_model'] = score_data.get('eval_llm_model', '')
    
    # ===== è¯„åˆ†æŒ‡æ ‡ =====
    scores = score_data.get('scores', {})
    
    # LLM è¯„åˆ†ï¼ˆä¸»è¯„åˆ†ï¼‰
    metrics['llm_accuracy'] = scores.get('llm_accuracy', None)
    
    # å…¶ä»–è¯„åˆ†
    metrics['exact_match'] = scores.get('exact_match', None)
    metrics['token_f1'] = scores.get('token_f1', None)
    metrics['rouge1_f'] = scores.get('rouge1_f', None)
    metrics['rouge2_f'] = scores.get('rouge2_f', None)
    metrics['rougeL_f'] = scores.get('rougeL_f', None)
    metrics['semantic_similarity'] = scores.get('semantic_similarity', None)
    metrics['avg_lexical'] = scores.get('avg_lexical', None)
    metrics['avg_semantic'] = scores.get('avg_semantic', None)
    metrics['overall_average'] = scores.get('overall_average', None)
    
    # LLM è¯„ä¼°è¯¦æƒ…
    evaluation = score_data.get('evaluation', {})
    llm_details = evaluation.get('llm_details', {})
    metrics['llm_consistency'] = llm_details.get('consistency', None)
    metrics['llm_confidence'] = llm_details.get('confidence', '')
    
    # ===== Token å ç”¨ =====
    gen_token = score_data.get('gen_token_usage', {})
    metrics['prompt_tokens'] = gen_token.get('prompt_tokens', None)
    metrics['answer_tokens'] = gen_token.get('answer_tokens', None)
    metrics['total_tokens'] = gen_token.get('total_tokens', None)
    metrics['context_length'] = gen_token.get('context_length', None)
    metrics['max_context_tokens'] = gen_token.get('max_context_tokens', None)
    metrics['prompt_ratio'] = gen_token.get('prompt_ratio', None)
    metrics['tokenizer_type'] = gen_token.get('tokenizer_type', '')
    metrics['encoding'] = gen_token.get('encoding', '')
    metrics['prompt_length_chars'] = score_data.get('prompt_length', None)
    
    # ===== æ—¶é—´ç»Ÿè®¡ =====
    timing = score_data.get('timing', {})
    metrics['load_time'] = timing.get('load_time', None)
    metrics['retrieval_time'] = timing.get('retrieval_time', None)
    metrics['generation_time'] = timing.get('generation_time', None)
    metrics['evaluation_time'] = timing.get('evaluation_time', None)
    metrics['cleanup_time'] = timing.get('cleanup_time', None)
    metrics['total_time'] = timing.get('total_time', None)
    
    # ===== æ£€ç´¢ä¿¡æ¯ =====
    metrics['query_top_k'] = retrieval_data.get('query_top_k', None)
    metrics['retrieved_memories_count'] = retrieval_data.get('retrieved_memories_count', None)
    
    load_result = retrieval_data.get('load_result', {})
    metrics['total_sessions'] = load_result.get('total_sessions', None)
    metrics['added_sessions'] = load_result.get('added_sessions', None)
    metrics['failed_sessions'] = load_result.get('failed_sessions', None)
    
    return metrics


def aggregate_results(results_dir: str, output_file: str = None) -> pd.DataFrame:
    """
    æ±‡æ€»ç›®å½•ä¸‹æ‰€æœ‰ QA ç»“æœ
    
    Args:
        results_dir: ç»“æœç›®å½•è·¯å¾„
        output_file: è¾“å‡º Excel æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        æ±‡æ€»çš„ DataFrame
    """
    results_path = Path(results_dir)
    
    if not results_path.exists():
        raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {results_dir}")
    
    # æŸ¥æ‰¾æ‰€æœ‰ QA_X ç›®å½•
    qa_dirs = sorted(
        [d for d in results_path.iterdir() if d.is_dir() and d.name.startswith('QA_')],
        key=lambda x: int(x.name[3:]) if x.name[3:].isdigit() else 0
    )
    
    print(f"æ‰¾åˆ° {len(qa_dirs)} ä¸ª QA ç›®å½•")
    
    # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
    all_metrics = []
    
    for qa_dir in qa_dirs:
        result = load_qa_results(qa_dir)
        metrics = extract_metrics(result)
        all_metrics.append(metrics)
    
    # åˆ›å»º DataFrame
    df = pd.DataFrame(all_metrics)
    
    # æŒ‰ qa_index æ’åº
    if 'qa_index' in df.columns:
        df = df.sort_values('qa_index').reset_index(drop=True)
    
    return df


def generate_summary_stats(df: pd.DataFrame) -> pd.DataFrame:
    """
    ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
    
    Args:
        df: åŸå§‹æ•°æ® DataFrame
        
    Returns:
        ç»Ÿè®¡æ‘˜è¦ DataFrame
    """
    stats = {}
    
    # æˆåŠŸ/å¤±è´¥ç»Ÿè®¡
    stats['æ€»æ ·æœ¬æ•°'] = len(df)
    stats['æˆåŠŸæ•°'] = len(df[df['status'] == 'success'])
    stats['å¤±è´¥æ•°'] = len(df[df['status'] != 'success'])
    stats['æˆåŠŸç‡'] = stats['æˆåŠŸæ•°'] / stats['æ€»æ ·æœ¬æ•°'] if stats['æ€»æ ·æœ¬æ•°'] > 0 else 0
    
    # ä»…å¯¹æˆåŠŸæ ·æœ¬è®¡ç®—ç»Ÿè®¡
    success_df = df[df['status'] == 'success']
    
    if len(success_df) > 0:
        # LLM è¯„åˆ†ç»Ÿè®¡ï¼ˆä¸»è¯„åˆ†ï¼‰
        if 'llm_accuracy' in success_df.columns:
            llm_scores = success_df['llm_accuracy'].dropna()
            stats['LLMå‡†ç¡®ç‡_å¹³å‡'] = llm_scores.mean()
            stats['LLMå‡†ç¡®ç‡_ä¸­ä½æ•°'] = llm_scores.median()
            stats['LLMå‡†ç¡®ç‡_æ ‡å‡†å·®'] = llm_scores.std()
            stats['LLMæ­£ç¡®æ•°'] = (llm_scores == 1.0).sum()
            stats['LLMæ­£ç¡®ç‡'] = stats['LLMæ­£ç¡®æ•°'] / len(llm_scores) if len(llm_scores) > 0 else 0
        
        # å…¶ä»–è¯„åˆ†ç»Ÿè®¡
        score_cols = ['exact_match', 'token_f1', 'rouge1_f', 'rougeL_f', 
                      'semantic_similarity', 'overall_average']
        for col in score_cols:
            if col in success_df.columns:
                values = success_df[col].dropna()
                if len(values) > 0:
                    stats[f'{col}_å¹³å‡'] = values.mean()
        
        # Token ç»Ÿè®¡
        token_cols = ['prompt_tokens', 'answer_tokens', 'total_tokens']
        for col in token_cols:
            if col in success_df.columns:
                values = success_df[col].dropna()
                if len(values) > 0:
                    stats[f'{col}_å¹³å‡'] = values.mean()
                    stats[f'{col}_æ€»è®¡'] = values.sum()
        
        # æ—¶é—´ç»Ÿè®¡
        time_cols = ['load_time', 'retrieval_time', 'generation_time', 
                     'evaluation_time', 'total_time']
        for col in time_cols:
            if col in success_df.columns:
                values = success_df[col].dropna()
                if len(values) > 0:
                    stats[f'{col}_å¹³å‡(ç§’)'] = values.mean()
                    stats[f'{col}_æ€»è®¡(ç§’)'] = values.sum()
        
        # æ£€ç´¢ç»Ÿè®¡
        if 'retrieved_memories_count' in success_df.columns:
            values = success_df['retrieved_memories_count'].dropna()
            if len(values) > 0:
                stats['æ£€ç´¢è®°å¿†æ•°_å¹³å‡'] = values.mean()
    
    # æŒ‰é—®é¢˜ç±»å‹ç»Ÿè®¡ LLM å‡†ç¡®ç‡
    if 'question_type' in success_df.columns and 'llm_accuracy' in success_df.columns:
        for q_type in success_df['question_type'].unique():
            if pd.notna(q_type):
                type_df = success_df[success_df['question_type'] == q_type]
                type_scores = type_df['llm_accuracy'].dropna()
                if len(type_scores) > 0:
                    stats[f'LLMå‡†ç¡®ç‡_{q_type}'] = type_scores.mean()
                    stats[f'æ ·æœ¬æ•°_{q_type}'] = len(type_scores)
    
    # è½¬æ¢ä¸º DataFrame
    stats_df = pd.DataFrame([stats]).T
    stats_df.columns = ['å€¼']
    stats_df.index.name = 'æŒ‡æ ‡'
    
    return stats_df


def format_worksheet(ws, is_summary: bool = False):
    """
    æ ¼å¼åŒ–å·¥ä½œè¡¨
    
    Args:
        ws: openpyxl worksheet å¯¹è±¡
        is_summary: æ˜¯å¦ä¸ºç»Ÿè®¡æ‘˜è¦è¡¨
    """
    # å®šä¹‰æ ·å¼
    header_font = Font(bold=True, size=11, color='FFFFFF')
    header_fill = PatternFill(start_color='4472C4', end_color='4472C4', fill_type='solid')
    header_alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)
    
    cell_alignment = Alignment(horizontal='left', vertical='center', wrap_text=True)
    number_alignment = Alignment(horizontal='right', vertical='center')
    
    thin_border = Border(
        left=Side(style='thin', color='B4B4B4'),
        right=Side(style='thin', color='B4B4B4'),
        top=Side(style='thin', color='B4B4B4'),
        bottom=Side(style='thin', color='B4B4B4')
    )
    
    # è®¾ç½®è¡Œé«˜
    for row in ws.iter_rows():
        ws.row_dimensions[row[0].row].height = 22 if not is_summary else 28
    
    # æ ¼å¼åŒ–è¡¨å¤´ï¼ˆç¬¬ä¸€è¡Œï¼‰
    for cell in ws[1]:
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = header_alignment
        cell.border = thin_border
    
    # æ ¼å¼åŒ–æ•°æ®åŒºåŸŸ
    for row_idx, row in enumerate(ws.iter_rows(min_row=2), start=2):
        # å¶æ•°è¡Œæ·»åŠ æµ…è‰²èƒŒæ™¯
        if row_idx % 2 == 0:
            row_fill = PatternFill(start_color='E9EFF7', end_color='E9EFF7', fill_type='solid')
        else:
            row_fill = PatternFill(start_color='FFFFFF', end_color='FFFFFF', fill_type='solid')
        
        for cell in row:
            cell.border = thin_border
            cell.fill = row_fill
            
            # æ ¹æ®æ•°æ®ç±»å‹è®¾ç½®å¯¹é½
            if isinstance(cell.value, (int, float)):
                cell.alignment = number_alignment
                # æ ¼å¼åŒ–æ•°å­—
                if isinstance(cell.value, float):
                    if abs(cell.value) < 1 and cell.value != 0:
                        cell.number_format = '0.0000'
                    else:
                        cell.number_format = '#,##0.00'
            else:
                cell.alignment = cell_alignment
    
    # è‡ªåŠ¨è°ƒæ•´åˆ—å®½
    for column_cells in ws.columns:
        max_length = 0
        column = column_cells[0].column_letter
        
        for cell in column_cells:
            try:
                if cell.value:
                    # è®¡ç®—æ˜¾ç¤ºé•¿åº¦ï¼ˆä¸­æ–‡å­—ç¬¦ç®—2ä¸ªå®½åº¦ï¼‰
                    cell_str = str(cell.value)
                    length = sum(2 if ord(c) > 127 else 1 for c in cell_str)
                    max_length = max(max_length, length)
            except:
                pass
        
        # è®¾ç½®åˆ—å®½ï¼Œæœ‰æœ€å°å’Œæœ€å¤§é™åˆ¶
        if is_summary:
            # ç»Ÿè®¡æ‘˜è¦è¡¨åˆ—å®½æ›´å¤§
            adjusted_width = max(min(max_length + 4, 60), 25)
        else:
            adjusted_width = max(min(max_length + 2, 50), 10)
        
        ws.column_dimensions[column].width = adjusted_width


def format_summary_worksheet(ws):
    """
    ä¸“é—¨æ ¼å¼åŒ–ç»Ÿè®¡æ‘˜è¦å·¥ä½œè¡¨
    
    Args:
        ws: openpyxl worksheet å¯¹è±¡
    """
    # å®šä¹‰æ ·å¼
    header_font = Font(bold=True, size=12, color='FFFFFF')
    header_fill = PatternFill(start_color='2E75B6', end_color='2E75B6', fill_type='solid')
    
    category_font = Font(bold=True, size=11, color='1F4E79')
    category_fill = PatternFill(start_color='BDD7EE', end_color='BDD7EE', fill_type='solid')
    
    normal_font = Font(size=11)
    value_font = Font(size=11, bold=True, color='C00000')
    
    cell_alignment = Alignment(horizontal='left', vertical='center', wrap_text=True)
    value_alignment = Alignment(horizontal='right', vertical='center')
    
    thick_border = Border(
        left=Side(style='medium', color='2E75B6'),
        right=Side(style='medium', color='2E75B6'),
        top=Side(style='medium', color='2E75B6'),
        bottom=Side(style='medium', color='2E75B6')
    )
    
    thin_border = Border(
        left=Side(style='thin', color='B4B4B4'),
        right=Side(style='thin', color='B4B4B4'),
        top=Side(style='thin', color='B4B4B4'),
        bottom=Side(style='thin', color='B4B4B4')
    )
    
    # è®¾ç½®åˆ—å®½
    ws.column_dimensions['A'].width = 35  # æŒ‡æ ‡åç§°åˆ—
    ws.column_dimensions['B'].width = 25  # å€¼åˆ—
    
    # å®šä¹‰åˆ†ç±»å…³é”®è¯
    category_keywords = {
        'åŸºæœ¬ç»Ÿè®¡': ['æ€»æ ·æœ¬æ•°', 'æˆåŠŸæ•°', 'å¤±è´¥æ•°', 'æˆåŠŸç‡'],
        'LLMè¯„åˆ†': ['LLMå‡†ç¡®ç‡', 'LLMæ­£ç¡®æ•°', 'LLMæ­£ç¡®ç‡'],
        'å…¶ä»–è¯„åˆ†': ['exact_match', 'token_f1', 'rouge', 'semantic', 'overall'],
        'Tokenç»Ÿè®¡': ['tokens_å¹³å‡', 'tokens_æ€»è®¡', 'prompt_tokens', 'answer_tokens', 'total_tokens'],
        'æ—¶é—´ç»Ÿè®¡': ['time_å¹³å‡', 'time_æ€»è®¡', 'load_time', 'retrieval_time', 'generation_time', 'evaluation_time', 'total_time'],
        'æ£€ç´¢ç»Ÿè®¡': ['æ£€ç´¢è®°å¿†æ•°'],
        'é—®é¢˜ç±»å‹': ['æ ·æœ¬æ•°_']
    }
    
    # æ ¼å¼åŒ–è¡¨å¤´
    for cell in ws[1]:
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = Alignment(horizontal='center', vertical='center')
        cell.border = thick_border
    ws.row_dimensions[1].height = 30
    
    # æ ¼å¼åŒ–æ•°æ®è¡Œ
    for row_idx, row in enumerate(ws.iter_rows(min_row=2), start=2):
        ws.row_dimensions[row_idx].height = 26
        
        metric_name = str(row[0].value) if row[0].value else ''
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†ç±»æ ‡é¢˜è¡Œï¼ˆé€šè¿‡å…³é”®è¯åˆ¤æ–­ï¼‰
        is_category_start = False
        for cat_name, keywords in category_keywords.items():
            for keyword in keywords:
                if keyword in metric_name:
                    is_category_start = True
                    break
            if is_category_start:
                break
        
        for col_idx, cell in enumerate(row):
            cell.border = thin_border
            
            if col_idx == 0:  # æŒ‡æ ‡åç§°åˆ—
                cell.font = normal_font
                cell.alignment = cell_alignment
            else:  # å€¼åˆ—
                cell.alignment = value_alignment
                
                # æ ¼å¼åŒ–æ•°å€¼
                if isinstance(cell.value, float):
                    if 'rate' in metric_name.lower() or 'ç‡' in metric_name or 'å‡†ç¡®' in metric_name:
                        # ç™¾åˆ†æ¯”æ ¼å¼
                        cell.number_format = '0.00%'
                        cell.font = Font(size=11, bold=True, color='008000')  # ç»¿è‰²
                    elif abs(cell.value) < 1 and cell.value != 0:
                        cell.number_format = '0.0000'
                        cell.font = value_font
                    else:
                        cell.number_format = '#,##0.00'
                        cell.font = value_font
                elif isinstance(cell.value, int):
                    cell.number_format = '#,##0'
                    cell.font = value_font
        
        # å¶æ•°è¡Œæ·»åŠ æµ…è‰²èƒŒæ™¯
        if row_idx % 2 == 0:
            for cell in row:
                cell.fill = PatternFill(start_color='F2F2F2', end_color='F2F2F2', fill_type='solid')


def save_to_excel(
    df: pd.DataFrame, 
    stats_df: pd.DataFrame, 
    output_file: str,
    results_dir: str
):
    """
    ä¿å­˜ç»“æœåˆ° Excel æ–‡ä»¶ï¼ˆå¸¦æ ¼å¼åŒ–ï¼‰
    
    Args:
        df: è¯¦ç»†æ•°æ® DataFrame
        stats_df: ç»Ÿè®¡æ‘˜è¦ DataFrame  
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        results_dir: åŸå§‹ç»“æœç›®å½•ï¼ˆç”¨äºè®°å½•å…ƒä¿¡æ¯ï¼‰
    """
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        # Sheet 1: ç»Ÿè®¡æ‘˜è¦
        stats_df.to_excel(writer, sheet_name='ç»Ÿè®¡æ‘˜è¦')
        
        # Sheet 2: è¯¦ç»†æ•°æ® - æ ¸å¿ƒæŒ‡æ ‡
        core_cols = [
            'qa_index', 'status', 'question_id', 'question_type',
            'llm_accuracy', 'exact_match', 'token_f1', 'semantic_similarity',
            'prompt_tokens', 'answer_tokens', 'total_tokens',
            'load_time', 'retrieval_time', 'generation_time', 'total_time',
            'retrieved_memories_count', 'score_error', 'retrieval_error'
        ]
        core_df = df[[c for c in core_cols if c in df.columns]]
        core_df.to_excel(writer, sheet_name='æ ¸å¿ƒæŒ‡æ ‡', index=False)
        
        # Sheet 3: è¯¦ç»†æ•°æ® - æ‰€æœ‰è¯„åˆ†
        score_cols = [
            'qa_index', 'question_id', 'question_type',
            'llm_accuracy', 'llm_consistency', 'llm_confidence',
            'exact_match', 'token_f1', 
            'rouge1_f', 'rouge2_f', 'rougeL_f',
            'semantic_similarity', 'avg_lexical', 'avg_semantic', 'overall_average'
        ]
        score_df = df[[c for c in score_cols if c in df.columns]]
        score_df.to_excel(writer, sheet_name='è¯„åˆ†è¯¦æƒ…', index=False)
        
        # Sheet 4: Token å’Œæ—¶é—´è¯¦æƒ…
        token_time_cols = [
            'qa_index', 'question_id',
            'prompt_tokens', 'answer_tokens', 'total_tokens',
            'prompt_length_chars', 'prompt_ratio',
            'load_time', 'retrieval_time', 'generation_time', 
            'evaluation_time', 'cleanup_time', 'total_time'
        ]
        token_time_df = df[[c for c in token_time_cols if c in df.columns]]
        token_time_df.to_excel(writer, sheet_name='Tokenä¸æ—¶é—´', index=False)
        
        # Sheet 5: é—®ç­”å†…å®¹
        qa_cols = [
            'qa_index', 'question_id', 'question_type', 'question_date',
            'question', 'gold_answer', 'predicted_answer',
            'llm_accuracy', 'gen_llm_model', 'eval_llm_model'
        ]
        qa_df = df[[c for c in qa_cols if c in df.columns]]
        qa_df.to_excel(writer, sheet_name='é—®ç­”å†…å®¹', index=False)
        
        # Sheet 6: å®Œæ•´æ•°æ®
        df.to_excel(writer, sheet_name='å®Œæ•´æ•°æ®', index=False)
        
        # Sheet 7: å…ƒä¿¡æ¯
        meta_info = {
            'ç»“æœç›®å½•': [results_dir],
            'ç”Ÿæˆæ—¶é—´': [datetime.now().isoformat()],
            'QAæ€»æ•°': [len(df)],
            'æˆåŠŸæ•°': [len(df[df['status'] == 'success'])],
        }
        if 'gen_llm_model' in df.columns:
            meta_info['ç”Ÿæˆæ¨¡å‹'] = [df['gen_llm_model'].iloc[0] if len(df) > 0 else '']
        if 'eval_llm_model' in df.columns:
            meta_info['è¯„ä¼°æ¨¡å‹'] = [df['eval_llm_model'].iloc[0] if len(df) > 0 else '']
        
        meta_df = pd.DataFrame(meta_info).T
        meta_df.columns = ['å€¼']
        meta_df.index.name = 'ä¿¡æ¯'
        meta_df.to_excel(writer, sheet_name='å…ƒä¿¡æ¯')
        
        # è·å– workbook å¹¶æ ¼å¼åŒ–å„ä¸ªå·¥ä½œè¡¨
        workbook = writer.book
        
        # æ ¼å¼åŒ–ç»Ÿè®¡æ‘˜è¦ï¼ˆç‰¹æ®Šå¤„ç†ï¼‰
        format_summary_worksheet(workbook['ç»Ÿè®¡æ‘˜è¦'])
        
        # æ ¼å¼åŒ–å…¶ä»–å·¥ä½œè¡¨
        for sheet_name in ['æ ¸å¿ƒæŒ‡æ ‡', 'è¯„åˆ†è¯¦æƒ…', 'Tokenä¸æ—¶é—´', 'é—®ç­”å†…å®¹', 'å®Œæ•´æ•°æ®', 'å…ƒä¿¡æ¯']:
            if sheet_name in workbook.sheetnames:
                format_worksheet(workbook[sheet_name], is_summary=(sheet_name == 'å…ƒä¿¡æ¯'))
    
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ±‡æ€» LongMemEval Benchmark å®éªŒæ•°æ®ï¼Œè¾“å‡º Excel è¡¨æ ¼'
    )
    parser.add_argument(
        '--input-dir', '-i',
        type=str,
        required=True,
        help='ç»“æœç›®å½•è·¯å¾„ï¼ˆåŒ…å« QA_X å­ç›®å½•ï¼‰'
    )
    parser.add_argument(
        '--output', '-o',
        type=str,
        default=None,
        help='è¾“å‡º Excel æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä¿å­˜åˆ° benchmark_longmemeval/benchmark_results/ï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='benchmark_longmemeval/benchmark_results',
        help='è¾“å‡ºç›®å½•ï¼ˆå½“ --output æœªæŒ‡å®šæ—¶ä½¿ç”¨ï¼‰'
    )
    parser.add_argument(
        '--print-summary',
        action='store_true',
        help='æ‰“å°ç»Ÿè®¡æ‘˜è¦åˆ°ç»ˆç«¯'
    )
    
    args = parser.parse_args()
    
    # æ±‡æ€»ç»“æœ
    print(f"æ­£åœ¨è¯»å–ç›®å½•: {args.input_dir}")
    df = aggregate_results(args.input_dir)
    
    print(f"å…±è¯»å– {len(df)} æ¡è®°å½•")
    print(f"æˆåŠŸ: {len(df[df['status'] == 'success'])}, å¤±è´¥: {len(df[df['status'] != 'success'])}")
    
    # ç”Ÿæˆç»Ÿè®¡
    stats_df = generate_summary_stats(df)
    
    # æ‰“å°æ‘˜è¦
    if args.print_summary:
        print("\n" + "="*60)
        print("ğŸ“Š ç»Ÿè®¡æ‘˜è¦")
        print("="*60)
        print(stats_df.to_string())
        print("="*60)
    
    # ================= æ‰“å°å¤±è´¥æ ·æœ¬è¯¦æƒ… =================
    failed_df = df[df['status'] != 'success']
    if len(failed_df) > 0:
        print("\n" + "="*60)
        print(f"âš ï¸  å‘ç° {len(failed_df)} ä¸ªå¤±è´¥æ ·æœ¬ (Failed Samples)")
        print("="*60)
        # æŒ‰ qa_index æ’åºè¾“å‡º
        failed_df = failed_df.sort_values('qa_index')
        for _, row in failed_df.iterrows():
            qa_idx = row.get('qa_index', 'N/A')
            q_id = row.get('question_id') or 'Unknown'
            status = row.get('status', 'Unknown')
            
            print(f"Directory: QA_{qa_idx}")
            print(f"ID       : {q_id}")
            print(f"Status   : {status}")
            
            # å¦‚æœæœ‰å…·ä½“çš„é”™è¯¯ä¿¡æ¯ï¼Œåˆ™æ‰“å°
            score_err = row.get('score_error')
            retr_err = row.get('retrieval_error')
            
            if score_err:
                print(f"Score Error: {score_err}")
            if retr_err:
                print(f"Retr Error : {retr_err}")
                
            print("-" * 30)
    # ===================================================

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    if args.output:
        output_file = args.output
    else:
        # ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        input_dir_name = Path(args.input_dir).name
        output_file = str(output_dir / f"benchmark_summary_{input_dir_name}_{timestamp}.xlsx")
    
    # ä¿å­˜åˆ° Excel
    save_to_excel(df, stats_df, output_file, args.input_dir)
    
    # æ‰“å°å…³é”®æŒ‡æ ‡
    success_df = df[df['status'] == 'success']
    if len(success_df) > 0 and 'llm_accuracy' in success_df.columns:
        llm_scores = success_df['llm_accuracy'].dropna()
        print(f"\nğŸ¯ LLM è¯„ä¼°å‡†ç¡®ç‡: {llm_scores.mean():.2%} ({(llm_scores == 1.0).sum()}/{len(llm_scores)} æ­£ç¡®)")
    
    if len(success_df) > 0 and 'total_time' in success_df.columns:
        total_time = success_df['total_time'].sum()
        avg_time = success_df['total_time'].mean()
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’, å¹³å‡: {avg_time:.2f}ç§’/æ ·æœ¬")
    
    if len(success_df) > 0 and 'total_tokens' in success_df.columns:
        total_tokens = success_df['total_tokens'].sum()
        print(f"ğŸ“ æ€» Token æ•°: {total_tokens:,.0f}")
    
    return df, stats_df


if __name__ == "__main__":
    main()